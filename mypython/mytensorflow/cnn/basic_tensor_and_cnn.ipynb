{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tensorflow.python.framework.ops.Graph object at 0x10dfc6a10>\n",
      "<tensorflow.python.framework.ops.Graph object at 0x10dfc6a10>\n",
      "<tensorflow.python.framework.ops.Graph object at 0x10dfc6a10>\n",
      "<tensorflow.python.framework.ops.Graph object at 0x10dfc6a10>\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "###################################################################################################\n",
    "# 计算图\n",
    "import tensorflow as tf\n",
    "\n",
    "a = tf.constant([1.0, 2.0], name = \"a\")\n",
    "b = tf.constant([2.0, 3.0], name = \"b\")\n",
    "\n",
    "result = a + b\n",
    "\n",
    "# 通过张量a.graph 可以查看张量所属的计算图。\n",
    "# 因为没有指定，所以这个计算图应该等于当前默认的计算图\n",
    "print (a.graph)\n",
    "print tf.get_default_graph()\n",
    "print b.graph\n",
    "print result.graph\n",
    "\n",
    "print a.graph is tf.get_default_graph()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.]\n",
      "[ 1.]\n"
     ]
    }
   ],
   "source": [
    "#tensorflow 只用tf.Graph生成新的计算图，不同的计算图上的张量和运算都不会共享\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# 在计算图g1种定义变量“v”， 并设置初始值为0\n",
    "g1 = tf.Graph()\n",
    "with g1.as_default():\n",
    "    v = tf.get_variable(\"v\", initializer=tf.zeros_initializer(shape=[1]))\n",
    "\n",
    "# 在计算图g2种定义变量“v”， 并设置初始值为1\n",
    "g2 = tf.Graph()\n",
    "with g2.as_default():\n",
    "    v = tf.get_variable(\"v\", initializer=tf.ones_initializer(shape=[1]))\n",
    "    \n",
    "# 在计算图g1种读取变量“v”的值\n",
    "with tf.Session(graph=g1) as sess:\n",
    "    tf.initialize_all_variables().run()\n",
    "    with tf.variable_scope(\"\", reuse=True):\n",
    "        print sess.run(tf.get_variable(\"v\"))\n",
    "        \n",
    "\n",
    "# 在计算图g2种读取变量“v”的值\n",
    "with tf.Session(graph=g2) as sess:\n",
    "    tf.initialize_all_variables().run()\n",
    "    with tf.variable_scope(\"\", reuse=True):\n",
    "        print sess.run(tf.get_variable(\"v\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 3.  5.]\n",
      "[ 3.  5.]\n",
      "[ 3.  5.]\n",
      "-----------------------------------------------------\n",
      "[ 3.  5.]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "__exit__",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-ff6700106b9c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInteractiveSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: __exit__"
     ]
    }
   ],
   "source": [
    "###################################################################################################\n",
    "# tensorflow会话\n",
    "# Session\n",
    "import tensorflow as tf\n",
    "\n",
    "a = tf.constant([1.0, 2.0], name = \"a\")\n",
    "b = tf.constant([2.0, 3.0], name = \"b\")\n",
    "\n",
    "result = a + b\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # 默认的会话被指定后，可以通过tf.Tensor.eval函数来计算一个张量的取值\n",
    "    with sess.as_default():\n",
    "        print result.eval()\n",
    "\n",
    "# 下面两个命令有相同的功能\n",
    "sess = tf.Session()\n",
    "print sess.run(result)\n",
    "print result.eval(session=sess)\n",
    "sess.close()\n",
    "\n",
    "print \"-----------------------------------------------------\"\n",
    "###################################################################################################\n",
    "# 通过tf.InteractiveSession函数可以省去将产生的会话注册为默认会话的过程,不能使用with\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "print result.eval()\n",
    "sess.close()\n",
    "\n",
    "with tf.InteractiveSession() as sess:\n",
    "    print result.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 3.  5.]\n",
      "[ 3.  5.]\n"
     ]
    }
   ],
   "source": [
    "# 通过ConfigProto配置会话\n",
    "\"\"\"\n",
    "    allow_soft_placement: GPU 运算可以放到CPU上运行，满足下面条件之一\n",
    "        1. 运算无法在GPU上运行\n",
    "        2. 没有GPU资源\n",
    "        3. 运算输入包含对CPU计算结果的引用\n",
    "    log_device_placement: 为True时，会记录每个节点被安排在哪个设备上\n",
    "        \n",
    "\"\"\"\n",
    "config = tf.ConfigProto(allow_soft_placement=True, log_device_placement=True)\n",
    "sess1 = tf.InteractiveSession(config=config)\n",
    "print result.eval()\n",
    "sess1.close()\n",
    "\n",
    "with tf.Session(config=config) as sess:\n",
    "    print sess.run(result)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3.95757794]]\n"
     ]
    }
   ],
   "source": [
    "# tensorflow使用api\n",
    "\"\"\"\n",
    "    1. TensorFlow中维护的集合列表\n",
    "        tf.GraphKeys.VARIABLES     所有变量\n",
    "        tf.GraphKeys.TRAINABLE_VARIABLES     可学习变量（神经网络中的参数）\n",
    "        tf.GraphKeys.SUMMARIES          日志生成的相关张量\n",
    "        tf.GraphKeys.QUEUE_RUNNERS       处理输入的QueueRunner\n",
    "        tf.GraphKeys.MOVING_AVERAGE_VARIABLES      所有计算滑动平均值的变量\n",
    "        \n",
    "    2. TensorFlow随机数生成函数\n",
    "        tf.random_normal  正太分布\n",
    "        tf.truncated_normal  正太分布（如果随机出来的值偏离平均值超过2个标准差，那么这个值会重新被随机）\n",
    "        tf.random_uniform  平均分布\n",
    "        tf.random_gamma     Gamma分布\n",
    "    3. TensorFlow生成函数\n",
    "        tf.zeros    产生全0数组\n",
    "        tf.ones     产生全1数组\n",
    "        tf.fill     产生一个全部为给定数字的数组\n",
    "        tf.constant 产生一个给定值的常量\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# 通过变量实现神经网络参数和前向传播过程\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "w1 = tf.Variable(tf.random_normal([2, 3], stddev=1, seed=1))\n",
    "w2 = tf.Variable(tf.random_normal([3, 1], stddev=1, seed=1))\n",
    "\n",
    "# 暂时将输入的特征向量定义为一个常量\n",
    "x = tf.constant([[0.7, 0.9]])\n",
    "\n",
    "# 通过前向传播算法获得神经网络的输出\n",
    "a = tf.matmul(x, w1)\n",
    "y = tf.matmul(a, w2)\n",
    "\n",
    "# 初始化w1，w2，并运行\n",
    "# 或者通过tf.initialize_all_variables()\n",
    "sess = tf.Session()\n",
    "sess.run(w1.initializer)\n",
    "sess.run(w2.initializer)\n",
    "print sess.run(y)\n",
    "sess.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "打印神经网络训练之前网络参数的值\n",
      "[[-0.81131822  1.48459876  0.06532937]\n",
      " [-2.44270396  0.0992484   0.59122431]]\n",
      "[[-0.81131822]\n",
      " [ 1.48459876]\n",
      " [ 0.06532937]]\n",
      "After 0 training step, cross entropy on all data is 0.0674925\n",
      "After 100 training step, cross entropy on all data is 0.0535382\n",
      "After 200 training step, cross entropy on all data is 0.0432842\n",
      "After 300 training step, cross entropy on all data is 0.0364409\n",
      "After 400 training step, cross entropy on all data is 0.0310934\n",
      "After 500 training step, cross entropy on all data is 0.0271219\n",
      "After 600 training step, cross entropy on all data is 0.0244868\n",
      "After 700 training step, cross entropy on all data is 0.0224556\n",
      "After 800 training step, cross entropy on all data is 0.0204244\n",
      "After 900 training step, cross entropy on all data is 0.0183805\n",
      "After 1000 training step, cross entropy on all data is 0.0163385\n",
      "After 1100 training step, cross entropy on all data is 0.0147953\n",
      "After 1200 training step, cross entropy on all data is 0.0139216\n",
      "After 1300 training step, cross entropy on all data is 0.0131479\n",
      "After 1400 training step, cross entropy on all data is 0.0123629\n",
      "After 1500 training step, cross entropy on all data is 0.0115551\n",
      "After 1600 training step, cross entropy on all data is 0.0107525\n",
      "After 1700 training step, cross entropy on all data is 0.010208\n",
      "After 1800 training step, cross entropy on all data is 0.00983513\n",
      "After 1900 training step, cross entropy on all data is 0.00945592\n",
      "After 2000 training step, cross entropy on all data is 0.00907547\n",
      "After 2100 training step, cross entropy on all data is 0.00868193\n",
      "After 2200 training step, cross entropy on all data is 0.00828326\n",
      "After 2300 training step, cross entropy on all data is 0.00794509\n",
      "After 2400 training step, cross entropy on all data is 0.00783947\n",
      "After 2500 training step, cross entropy on all data is 0.00773293\n",
      "After 2600 training step, cross entropy on all data is 0.00762075\n",
      "After 2700 training step, cross entropy on all data is 0.00750438\n",
      "After 2800 training step, cross entropy on all data is 0.00738713\n",
      "After 2900 training step, cross entropy on all data is 0.00726849\n",
      "After 3000 training step, cross entropy on all data is 0.00714436\n",
      "After 3100 training step, cross entropy on all data is 0.00701641\n",
      "After 3200 training step, cross entropy on all data is 0.00688828\n",
      "After 3300 training step, cross entropy on all data is 0.00675939\n",
      "After 3400 training step, cross entropy on all data is 0.00662533\n",
      "After 3500 training step, cross entropy on all data is 0.00648791\n",
      "After 3600 training step, cross entropy on all data is 0.00635107\n",
      "After 3700 training step, cross entropy on all data is 0.00621414\n",
      "After 3800 training step, cross entropy on all data is 0.00607244\n",
      "After 3900 training step, cross entropy on all data is 0.00592793\n",
      "After 4000 training step, cross entropy on all data is 0.00578471\n",
      "After 4100 training step, cross entropy on all data is 0.00564206\n",
      "After 4200 training step, cross entropy on all data is 0.00549509\n",
      "After 4300 training step, cross entropy on all data is 0.00534584\n",
      "After 4400 training step, cross entropy on all data is 0.00519853\n",
      "After 4500 training step, cross entropy on all data is 0.00505237\n",
      "After 4600 training step, cross entropy on all data is 0.00490233\n",
      "After 4700 training step, cross entropy on all data is 0.00475051\n",
      "After 4800 training step, cross entropy on all data is 0.00460116\n",
      "After 4900 training step, cross entropy on all data is 0.00445342\n",
      "打印训练后的神经网络参数值\n",
      "[[-1.9618274   2.58235407  1.68203783]\n",
      " [-3.4681716   1.06982327  2.11788988]]\n",
      "[[-1.8247149 ]\n",
      " [ 2.68546653]\n",
      " [ 1.41819501]]\n"
     ]
    }
   ],
   "source": [
    "# 神经网络完整实例1\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "import tensorflow as tf\n",
    "from numpy.random import RandomState\n",
    "\n",
    "###################################################################################################\n",
    "# 定义神经网络的参数，以及前向，反向传播算法\n",
    "\n",
    "# 定义训练数据的batch大小\n",
    "batch_size = 8\n",
    "\n",
    "# 定义神经网络参数\n",
    "w1 = tf.Variable(tf.random_normal([2, 3], stddev=1, seed=1))\n",
    "w2 = tf.Variable(tf.random_normal([3, 1], stddev=1, seed=1))\n",
    "\n",
    "# 定义输入数据集的placeholder\n",
    "# 在shape的一个维度上使用None，可以方便使用不大的batch大小，只适用于数据集比较小时\n",
    "x = tf.placeholder(tf.float32, shape=(None, 2), name=\"x-input\")\n",
    "y_ = tf.placeholder(tf.float32, shape=(None, 1), name=\"y-input\")\n",
    "\n",
    "# 定义神经网络前向传播过程\n",
    "a = tf.matmul(x, w1)\n",
    "y = tf.matmul(a, w2)\n",
    "\n",
    "# 定义损失函数和反向传播算法\n",
    "cross_entropy = - tf.reduce_mean(y_ * tf.log(tf.clip_by_value(y, 1e-10, 1.0)))\n",
    "train_step = tf.train.AdamOptimizer(0.001).minimize(cross_entropy)\n",
    "\n",
    "\n",
    "###################################################################################################\n",
    "# 通过RandomState随机数生成一个模拟数据集\n",
    "rdm = RandomState(1)\n",
    "dataset_size = 128\n",
    "X = rdm.rand(dataset_size, 2)\n",
    "# 定义规则给出样本标签。 x1 + x2 < 1 为正样本，否则为服样本\n",
    "Y = [[int (x1 + x2 < 1)] for (x1, x2) in X]\n",
    "\n",
    "\n",
    "###################################################################################################\n",
    "# TensorFlow训练过程\n",
    "with tf.Session() as sess:\n",
    "    # 初始化所以变量\n",
    "    init_op = tf.initialize_all_variables()\n",
    "    sess.run(init_op)\n",
    "    \n",
    "    # 打印神经网络训练之前网络参数的值\n",
    "    print '打印神经网络训练之前网络参数的值'\n",
    "    print sess.run(w1)\n",
    "    print sess.run(w2)\n",
    "    \"\"\"\n",
    "        [[-0.81131822  1.48459876  0.06532937]\n",
    "         [-2.44270396  0.0992484   0.59122431]]\n",
    "        [[-0.81131822]\n",
    "         [ 1.48459876]\n",
    "         [ 0.06532937]]\n",
    "    \"\"\"\n",
    "    \n",
    "    # 设定训练的轮数\n",
    "    STEPS = 5000\n",
    "    for i in range(STEPS):\n",
    "        # 每次选取batch_size个样本进行训练\n",
    "        start = (i * batch_size) % dataset_size\n",
    "        end = min (start + batch_size, dataset_size)\n",
    "        \n",
    "        # 通过选取的样本训练神经网络并更新参数\n",
    "        sess.run(train_step, feed_dict={x: X[start:end], y_: Y[start:end]})\n",
    "        \n",
    "        # 每隔一段时间计算在所有数据上的交叉商并输出\n",
    "        if i % 100 == 0:\n",
    "            total_cross_entropy = sess.run(cross_entropy, feed_dict={x: X, y_: Y})\n",
    "            print (\"After %d training step, cross entropy on all data is %g\" % (i, total_cross_entropy))\n",
    "            \n",
    "    \n",
    "    # 打印训练后的神经网络参数值\n",
    "    print '打印训练后的神经网络参数值'\n",
    "    print sess.run(w1)\n",
    "    print sess.run(w2)\n",
    "    \"\"\"\n",
    "        [[-1.9618274   2.58235407  1.68203783]\n",
    "         [-3.4681716   1.06982327  2.11788988]]\n",
    "        [[-1.8247149 ]\n",
    "         [ 2.68546653]\n",
    "         [ 1.41819501]]\n",
    "    \"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
