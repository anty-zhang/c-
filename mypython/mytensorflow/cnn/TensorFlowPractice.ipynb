{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 深度学习介绍\n",
    "\n",
    "## 深度学习／机器学习／人工智能\n",
    "\n",
    "\n",
    "## 深度学习发展历程\n",
    "\n",
    "## 深度学习的应用\n",
    "\n",
    "## 深度学习的工具\n",
    "\n",
    "\n",
    "\n",
    "# TensorFlow环境搭建\n",
    "\n",
    "## 依赖包\n",
    "\n",
    "- Protocol Buffer\n",
    "\n",
    "> 处理结构化数据： 在持久化或者进行网络传输时，需要将结构化的数据序列化，即将结构化的数据变成数据流的形式，简单来说就是变为一个字符串； 并将结构化的数据还原出来原来的结构化数据。\n",
    "\n",
    "> 优点： protocol buffer序列化后的数据是不可读的二进制流，相比json和xml格式数据小3-10倍，解析时间快20-100倍。\n",
    "\n",
    "- Bazel\n",
    "\n",
    "## install\n",
    "\n",
    "### docker安装\n",
    "\n",
    "\n",
    "### pip安装\n",
    "\n",
    "\n",
    "### 源码安装\n",
    "\n",
    "\n",
    "#### mac环境下安装\n",
    "\n",
    "```sh\n",
    "brew install bazel swig\n",
    "\n",
    "sudo easy_install -U six\n",
    "sudo easy_install -U numpy\n",
    "sudo easy_install wheel\n",
    "sudo easy_install ipython\n",
    "\n",
    "git clone https://github.com/tensorflow/tensorflow.git -b r0.9\n",
    "\n",
    "cd tensorflow\n",
    "./configure\n",
    "Please specify the location of python. [Default is /data/env/bin/python]:\n",
    "Do you wish to build TensorFlow with Google Cloud Platform support? [y/N] N\n",
    "No Google Cloud Platform support will be enabled for TensorFlow\n",
    "Do you wish to build TensorFlow with GPU support? [y/N] N\n",
    "No GPU support will be enabled for TensorFlow\n",
    "Configuration finished\n",
    "\n",
    "bazel build -c opt //tensorflow/tools/pip_package:build_pip_package\n",
    "./bazel-bin/tensorflow/tools/pip_package/build_pip_package ~/work5/tensorflow/tensorflow_pkg\n",
    "pip install ~/work5/tensorflow/tensorflow_pkg/tensorflow-0.9.0-py2-none-any.whl\n",
    "\n",
    "# 验证\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# r0.9编译遇到的问题\n",
    "# 1. tensorflow.bzl name 'DATA_CFG' is not defined\n",
    "# https://github.com/tensorflow/tensorflow/commit/7bcdcbbf60fc08346fd8016270a0563f4b51362b\n",
    "#  tensorflow/tensorflow.bzl \n",
    "# cfg = DATA_CFG 修改为 cfg = \"data\" \n",
    "# cfg = HOST_CFG 修改为 cfg = \"host\"\n",
    "\n",
    "\n",
    "# 2. depends on testonly target '//tensorflow/python:construction_fails_op' and doesn't have testonly attribute set\n",
    "# https://github.com/tensorflow/tensorflow/pull/5144/files\n",
    "\n",
    "# py_test(\n",
    "#      name = \"session_test\",\n",
    "#      size = \"small\",\n",
    "#      srcs = [\"client/session_test.py\"],\n",
    "#      srcs_version = \"PY2AND3\",\n",
    "#      deps = [\n",
    "#          \":array_ops\",\n",
    "# +        \":construction_fails_op\",\n",
    "#          \":control_flow_ops\",\n",
    "#          \":data_flow_ops\",\n",
    "#          \":framework\",# \n",
    "# \n",
    "# \n",
    "# \n",
    "\n",
    "#      srcs = [\"client/tf_session_helper.cc\"],\n",
    "#      hdrs = [\"client/tf_session_helper.h\"],\n",
    "#      deps = [\n",
    "# -        \":construction_fails_op\",\n",
    "#          \":numpy_lib\",\n",
    "# -        \":test_ops_kernels\",\n",
    "#          \"//tensorflow/c:c_api\",\n",
    "#          \"//tensorflow/c:tf_status_helper\",\n",
    "#          \"//tensorflow/core\",\n",
    "# @@ -1927,26 +1925,27 @@\n",
    "#      ],\n",
    "# )\n",
    "\n",
    "```\n",
    "\n",
    "```bash\n",
    "gpu安装\n",
    "\n",
    "# mac查看图形卡信息\n",
    "system_profiler SPDisplaysDataType\n",
    "\n",
    "# GNU coreutils\n",
    "brew install coreutils\n",
    "# Cuda Toolkit\n",
    "brew tap caskroom/cask\n",
    "brew cask install cuda\n",
    "\n",
    "# cuDNN\n",
    "https://developer.nvidia.com/cudnn\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "#### centos 7 环境下安装\n",
    "```sh\n",
    "\n",
    "rpm -qa | grep libarchive\n",
    "wget http://libarchive.org/downloads/libarchive-3.1.2.tar.gz\n",
    "tar -zxvf libarchive-3.1.2.tar.gz\n",
    "cd libarchive\n",
    "./configure\n",
    "sudo make install\n",
    "\n",
    "export  JAVA_HOME=/usr/java/jdk1.8.0_112\n",
    "git clone https://github.com/google/bazel/\n",
    "./compile.sh\n",
    "\n",
    "\n",
    "# cpu简单安装\n",
    "pip install --upgrade tensorflow\n",
    "\n",
    "# gpu 编译安装\n",
    "\n",
    "# cudnn 安装\n",
    "\n",
    "tar -zxvf cudnn-8.0-linux-x64-v6.0.tgz\n",
    "cp include/cudnn.h /usr/local/cuda-8.0/include/\n",
    "sudo cp include/cudnn.h /usr/local/cuda-8.0/include/\n",
    "sudo ln -s /usr/local/cuda-8.0/lib64/libcudnn* /usr/local/cuda/lib64/\n",
    "\n",
    "# 配置环境变量\n",
    "export CUDA_HOME=/usr/local/cuda-8.0\n",
    "export DYLD_LIBRARY_PATH=\"$DYLD_LIBRARY_PATH:$CUDA_HOME/lib\"\n",
    "export PATH=$CUDA_HOME/bin:$PATH\n",
    "\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.,  5.], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tensorflow 安装验证\n",
    "import tensorflow as tf\n",
    "a = tf.constant([1.0, 2.0], name = \"a\")\n",
    "b = tf.constant([2.0, 3.0], name = \"b\")\n",
    "result = a + b\n",
    "sess = tf.Session()\n",
    "sess.run(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "\n",
    "# TensorFlow基本概念模型\n",
    "\n",
    "## 概念\n",
    "\n",
    "### TensorFlow模型概念\n",
    "\n",
    "#### 计算模型---计算图\n",
    "\n",
    "- 计算图每个节点都是一个运算\n",
    "- 计算图上的边表示了运算之间的数据传递关系\n",
    "- 计算图上保存了每个运算的设备信息（CPU／GPU）\n",
    "- 计算图保存依赖关系\n",
    "- 计算图提供管理不同集合的功能，TensorFlow自动维护5个不同的默认集合\n",
    "- 隔离张量和计算（不同图上的张量和运算都不会共享）\n",
    "\n",
    "\n",
    "#### 数据模型---张量\n",
    "\n",
    "- 对中间结果引用\n",
    "- 获取计算结果\n",
    "- 张量本身不存储任何数据，只是对运算结果的引用\n",
    "- 张量结构\n",
    "\n",
    "> 三个属性： name（名字），shape（维度），type（类型）\n",
    "\n",
    "\n",
    "#### 运行模型---会话\n",
    "\n",
    "- 管理一个TensorFlow程序拥有的系统资源\n",
    "- 所有的运算都通过会话执行\n",
    "\n",
    "------\n",
    "\n",
    "###  其他概念\n",
    "\n",
    "- 全链接： 神经网络相邻两层之间任意两个节点之间都有连接\n",
    "- 隐藏层： 在输入和输出之间的神经网络叫做隐藏层\n",
    "\n",
    "- 监督学习最重要的思想： 在已知答案的标注数据集上，通过神经网络中的参数对训练数据进行拟合，可以得到模型对未知样本提供预测能力。\n",
    "\n",
    "\n",
    "## 前向传播算法简介\n",
    "\n",
    "### 神经网络前向传播结果需要3部分信息\n",
    "\n",
    "- 神经网络的输入： 即使从实体中提取特征向量\n",
    "- 神经网络的连接结构： 不同神经元（节点）之间输入输出的连接关系\n",
    "- 每个神经元中的参数\n",
    "\n",
    "\n",
    "## 训练神经网络过程的三个步骤\n",
    "\n",
    "- 定义神经网络的结构和前向传播的输出结果\n",
    "- 定义损失函数以及选择反向传播优化的算法\n",
    "- 生成会话，并且在训练数据上反复运行反向传播算法\n",
    "\n",
    "\n",
    "## TensorFlow游乐场\n",
    "\n",
    "http://playground.tensorflow.org/\n",
    "\n",
    "------\n",
    "\n",
    "# 神经网络优化的方法\n",
    "\n",
    "## 神经网络结构设计\n",
    "\n",
    "- 使用激活函数和多层隐藏层。\n",
    "- 神经网络的结构对最终模型的效果有本质的影响\n",
    "\n",
    "## 神经网络优化\n",
    "\n",
    "- 滑动平均模型和指数衰减的学习率： 在一定程度上都限制了神经网络中参数更新的速度。 但如果模型收敛的速度很快，这两种优化对模型的影响不大； 当问题更加复杂，且迭代不会快速收敛时，滑动平均模型和指数衰减的学习率可以发挥更大的作用。\n",
    "- 正则化：只优化交叉熵模型可以更好的拟合训练数据，但不能很好的发掘数据中潜在的规律判断未知的测试数据；\n",
    "\n",
    "> 当问题简单时：迭代后期的梯度非常小，正则化损失增长也不快，可以只考虑优化交叉熵损失；\n",
    "\n",
    "> 当问题复杂时，随着迭代后期的梯度更大，优化交叉熵损失和正则化损失的总损失，从而可以使得整体的损失呈现一个逐步递减的趋势，即使用正则化模型，可以使总损失呈现一个U型结构。\n",
    "\n",
    "------\n",
    "\n",
    "# 深层神经网络\n",
    "## 深度学习与深层神经网络\n",
    "\n",
    "- 深度学习： 一类通过多层非线性变换对高复杂性数据建模算法的集合\n",
    "\n",
    "### 线性模型的局限性及解决\n",
    "\n",
    "#### 线性局限性---激活函数\n",
    "- 线性模型的最大特点就是，任意的线形组合仍然是线性模型。比如，当模型的输入只有一个输入的时候，x和y形成了二维坐标系的一条直线；当有n个输入的时候，x和y形成了n+1维空间的一个平面。\n",
    "- 局限性：线性模型的特点决定了只通过线性变换，任意层的全链接神经网络和单层神经网络的表达能力没有任何区别。而且它们都是线性模型，而线性模型能够解决的问题是有限的，这就是线性模型的最大局限性\n",
    "- 激活函数：relu，sigmoid，tanh等\n",
    "\n",
    "#### 异或运算局限性---隐藏层\n",
    "- 感知机模型： 单层的神经网络---先将输入进行加权求和，然后通过激活函数最后得到输出，即没有隐藏层的神经网络结构。\n",
    "- 深层神经网络： 加入隐藏层后，异或问题得到解决。\n",
    "\n",
    "\n",
    "## 损失函数---神经网络优化目标和神经网络模型效果\n",
    "\n",
    "### 交叉熵---分类问题损失函数\n",
    "\n",
    "- 交叉熵： 刻画两个概率分布之间的距离。神经网络通过softmax回归的神经网络，将神经网络输出变成一个概率分布。\n",
    "\n",
    "> 假设原始的神经网络输出为y1, y2,..., yn，那么经过softmax回归处理后的输出结果为：\n",
    "\n",
    "> $$ softmax(y)_i = y_i{}' = \\frac{e^{yi}}{\\sum_{j=1}^{n}e^{yi}}$$\n",
    "\n",
    "> 通过q来表示p的交叉熵为： $$H(p, q) =\\sum p(x)log q(x)$$\n",
    "\n",
    "> 它刻画了通过概率分布q来表达概率分布p的困难程度。当交叉熵作为神经网络的损失函数，p代表的是正确答案，q代表的是预测值。交叉熵刻画了两个概率的分布距离，即，交叉熵值越小，两个概率分布越接近。\n",
    "\n",
    "\n",
    "### 均方误差（MSE，mean squared error）--- 回归问题\n",
    "\n",
    "- 回归问题是对具体数值做预测，比如房价预测，销量预测等，即预测的不是一个事先定义好的类别，而是一个任意的实数。\n",
    "\n",
    "> MSE 定义如下\n",
    "$$MSE(y, y{}') = \\frac{\\sum_{j=1}^{n}(y_i-y_i{}')}{n}$$\n",
    "\n",
    "\n",
    "### 自定义损失函数\n",
    "\n",
    "\n",
    "\n",
    "## 神经网络优化算法---反向传播算法\n",
    "\n",
    "\n",
    "\n",
    "### 反向传播算法的优化过程\n",
    "\n",
    "- 反向传播算法以高效的方式在所有的参数上使用梯度下降算法，从而使神经网络训练模型在训数据上的损失函数尽可能的小。神经网络模型中参数的优化过程直接决定了模型的质量。\n",
    "\n",
    "- 神经网络的优化过程可以分为两个阶段：(TODO 数学推导后续再研究)\n",
    "\n",
    "> 1. 先通过前向传播算法计算得到预测值，并将预测值和真实值之间做对比得出两者的差距\n",
    "> 2. 通过反向传播算法计算损失函数对每一个参数的梯度，再根据梯度和学习率使用梯度下降算法更新每一个参数。\n",
    "\n",
    "#### 1. 梯度下降算法\n",
    "- 梯度下降算法主要用于优化单个参数的取值。计算公式为\n",
    "> $$ \\theta_{n+1} = \\theta_n - \\eta\\frac{\\partial }{\\partial \\theta_n} J(\\theta) $$\n",
    "> $$\\theta : 神经网络中的参数 $$\n",
    "> $$ J(\\theta) : 在给定的参数下，训练数据集上损失函数大小， 整个的优化过程可以抽象寻找一个参数 \\theta， 使得J(\\theta)最小$$\n",
    "> $$\\eta： 为学习率$$\n",
    "\n",
    "- 梯度下降算法的局限\n",
    "\n",
    "> 1. 梯度下降算法不能保证全局最优. 参数的初始值在很大程度影响最后得到的结果\n",
    "\n",
    "> 2. 计算时间过长。因为要在全部训练数据上最小化损失，所以损失函数是在所有训练数据上的损失和，这样在每一轮迭代中需要计算在全部训练数据上的损失函数，海量数据上会使训练时间过长。\n",
    "\n",
    "- 避免梯度下降算法局限的方式\n",
    "\n",
    "> 1. 损失函数为凸函数时，参数的初始值会很大程度影响是否能够找到全局最优解。 只有当损失函数为凸函数时，梯度下降算法才能够保证达到全局最优解。\n",
    "\n",
    "> 2. 使用随机梯度下降算法加速训练过程。问题：使用随机梯度下降优化算法得到的神经网络可能无法达到全局最优。 原因： 随机梯度下降算法不是在全部训练数据上的损失函数，而是在每一轮迭代过程中，随机优化某一条训练数据上的损失函数，这将会导致在某一条数据上损失函数更小，不能代表在全部数据上的损失函数更小。\n",
    "\n",
    "> 3. 实际处理方式： 每次计算batch（小部分）训练数据的损失函数。 通过矩阵运算，每次在一个batch上优化神经网络的参数并不会比单个数据慢太多；每一次使用一个batch可以大大减小收敛的迭代次数。 同时，也能够使收敛的结果更加接近梯度下降的效果。\n",
    "\n",
    "\n",
    "\n",
    "#### 2. 学习率优化\n",
    "\n",
    "- 学习率设置存在的问题\n",
    "\n",
    "> 1. 学习率过大： 导致参数在极优值的两侧来回移动\n",
    "> 2. 学习率过小： 能够保证收敛，但会大大降低优化速度\n",
    "\n",
    "- tensorflow设置学习率的方式\n",
    "\n",
    "> 1. 通过指数衰减的方法设置梯度下降算法中的学习率。既可以使模型在训练前期快速接近较优解，又可以保证模型在训练后期不会有很大的波动，从而更加接近局部最优\n",
    "\n",
    "> 2. tf.train.exponential_decay 函数实现了下面代码的功能：\n",
    "> decayed_learning_rate = learning_rate * decay_rate^(global_step/decay_steps)\n",
    "\n",
    "\n",
    "#### 3. 正则化优化--- 解决过拟合\n",
    "\n",
    "- 过拟合： 当模型过为复杂后，它可以很好的“记忆”每个训练数据中的随机噪音部分， 而没有“学习”训练数据中的通用趋势。\n",
    "\n",
    "- 正则化的思想： 在损失函数中加入刻画模型复杂度的指标。\n",
    "\n",
    "> 1. $$正则化的优化目标：不是直接的J(\\theta)， 而是最优J(\\theta) + \\lambda R(w)。 其中 \\lambda 表示模型复杂损失在总损失中的比例，$$\n",
    "> $$ \\theta 表示神经网络中的所有参数，包括边上的权重w和偏置项b， 一半来说模型复杂度只有权重w决定, R(w)刻画模型复杂度$$\n",
    "\n",
    "> 2. $$L1 正则化公式：R(w) = \\parallel w\\parallel_1 = \\sum_i\\mid w_i \\mid $$\n",
    "> $$L1 正则化公式：R(w) = \\parallel w\\parallel_2^2 = \\sum_i\\mid w_i^2 \\mid$$\n",
    "> $$在实践中可以将L1和L2同时使用：R(w) = \\sum_i \\alpha \\mid w_i \\mid + (1 - \\alpha) w_i^2 $$\n",
    "\n",
    "> 3. L1和L2的区别： （1）L1 正则会让参数变得更稀疏，而L2不会。 特征稀疏指的是会有更的参数变为0，这样可以达到类似特征选取的功能。L2不会让参数变的稀疏的原因是当参数很小了，比如0.001，这个参数的平方基本可以忽略了，于是模型不会进一步将这个参数调整为0\n",
    "\n",
    "> (2) L1公式不可导，L2可以。因为在优化时需要计算损失函数的偏导数，所以对含有L2正则化损失函数的优化更加简洁，而优化带L1正则化的损失函数会更佳复杂，而且优化方法也有很多种\n",
    "\n",
    "#### 4. 滑动平均模型\n",
    "\n",
    "- 滑动平均模型可以使模型在测试数据上更健壮的一种方法。\n",
    "\n",
    "- 在TensorFlow中使用tf.train.ExponentialMovingAverage实现滑动平均模型。在初始化ExponentialMovingAverate时，需要提供一个衰减率(decay)用来控制模型更新的速度。\n",
    "\n",
    "> ExponentialMovingAverage对每一个变量会维护一个影子变量，每次运行变量更新时，影子变量的值会更新为：\n",
    "> shadow_variable = decay * shadow_variable + (1 - decay) * variable, 其中decay为衰减率，会设置成更接近1的数，比如0.999, 0.9999，variable为待更新的变量，shadow_variable为影子变量。\n",
    "\n",
    "\n",
    "> $$ ExponentialMovingAverage使用num_updates参数动态设置decay大小。 decay = \\left \\{ decay,  \\frac{1 + numupdates}{10 + numupdates} \\right \\}$$\n",
    "\n",
    "\n",
    "#### 5. 验证数据集\n",
    "\n",
    "- 交叉验证或者小批量验证数据集\n",
    "- 验证数据集：使用验证集来选取参数，而非测试数据，是因为通过测试数据选取参数可能会导致神经网络模型过渡拟合测试数据，而失去对未知数据的预判能力。\n",
    "- 可以通过模型在验证数据上的表现来判断一个模型的优劣。\n",
    "- 验证数据集和测试数据集的相关系数应该是趋于1的\n",
    "- 需要考虑验证数据集的数据分布情况\n",
    "\n",
    "## 神经网络优化中遇到的常见问题\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 其他学习资料\n",
    "\n",
    "http://www.deeplearningbook.org/ （Deep Learning An MIT Press book）\n",
    "\n",
    "http://neuralnetworksanddeeplearning.com/index.html （Ian Goodfellow, Yoshua Bengio, and Aaron Courville.）\n",
    "\n",
    "https://github.com/caicloud/tensorflow-tutorial.git (tensorflow 实战Google深度学习框架源码地址)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 循环神经网络\n",
    "\n",
    "## 循环神经网络的应用\n",
    "\n",
    "### 语言模型与文本生成（language modeling and Generating Text）\n",
    "\n",
    "> RNNs中的语言模型和文本生成研究的三篇文章\n",
    "\n",
    "> http://www.fit.vutbr.cz/research/groups/speech/publi/2010/mikolov_interspeech2010_IS100722.pdf (Recurrent neural network based language model)\n",
    "\n",
    "> http://www.fit.vutbr.cz/research/groups/speech/publi/2011/mikolov_icassp2011_5528.pdf (Extensions of Recurrent neural network based language model)\n",
    "\n",
    "> http://machinelearning.wustl.edu/mlpapers/paper_files/ICML2011Sutskever_524.pdf (Generating Text with Recurrent Neural Networks)\n",
    "\n",
    "### 机器翻译（Machine Translation）\n",
    "\n",
    "> RNNs中机器翻译研究的三篇文章\n",
    "\n",
    "> http://cs224d.stanford.edu/lectures/CS224d-Lecture8.pdf ()\n",
    "\n",
    "> http://www.aclweb.org/anthology/P14-1140.pdf (A Recursive Recurrent Neural Network for Statistical Machine Translation)\n",
    "\n",
    "> http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf (Sequence to Sequence Learning with Neural Networks)\n",
    "\n",
    "> http://research.microsoft.com/en-us/um/people/gzweig/Pubs/EMNLP2013RNNMT.pdf (Joint Language and Translation Modeling with Recurrent Neural Networks)\n",
    "\n",
    "### 语音识别（Speech Recongition）\n",
    "\n",
    "> 论文\n",
    "\n",
    "> http://proceedings.mlr.press/v32/graves14.pdf (Towards End-to-End Speech Recognition with Recurrent Neural Networks)\n",
    "\n",
    "### 图像生成（Generating Image Descriptions）\n",
    "\n",
    "> 将Cnns和Rnns组合可以根据图像的特征生成描述\n",
    "\n",
    "> http://cs.stanford.edu/people/karpathy/deepimagesent/\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 学习博客\n",
    "\n",
    "http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/\n",
    "\n",
    "http://blog.csdn.net/heyongluoyao8/article/details/48636251 （循环神经网络(RNN, Recurrent Neural Networks)介绍 ）\n",
    "\n",
    "https://en.wikipedia.org/wiki/Recurrent_neural_network#cite_note-4 (更多的RNN变种)\n",
    "\n",
    "http://www.wildml.com\n",
    "\n",
    "http://latex.codecogs.com/eqneditor/editor.php (math)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-edc7a88383a4>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-edc7a88383a4>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    https://www.analyticsvidhya.com/blog/2015/09/ultimate-data-scientists-world-today/\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# 25 Ultimate Data Scientist\n",
    "\n",
    "https://www.analyticsvidhya.com/blog/2015/09/ultimate-data-scientists-world-today/\n",
    "\n",
    "> 论文查找位置： 顶级会议与期刊发表了大量的论文，如Science、NIPS、ICML、ACL、CVPR、ICLR、IJCAI、ICPR"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
