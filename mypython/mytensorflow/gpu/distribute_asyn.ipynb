{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 分布式TensorFlow远离\n",
    "\n",
    "### 集群\n",
    "\n",
    "一个集群中的任务会聚合成jobs，每个jobs下会有很多任务\n",
    "\n",
    "\n",
    "\n",
    "### 图内分布式式\n",
    "\n",
    "> 在一台机器上多GPU的场景\n",
    "\n",
    "### 图间分布式\n",
    "\n",
    "##### 同步\n",
    "\n",
    "#### 异步\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "DEFINE_string() takes exactly 3 arguments (1 given)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-44ecc6ed1acd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;31m# 指定集群中计算服务器地址\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m tf.app.flags.DEFINE_string(('worker_hosts', 'tf-worker0:2222, tf-worker1:2223', 'Comma-separated list of hostname:port'\n\u001b[0m\u001b[1;32m     51\u001b[0m                                          ' for the worker jobs. e.g. \"tf-worker0:2222, tf-worker1:2223\"'))\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: DEFINE_string() takes exactly 3 arguments (1 given)"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# tensorflow异步程序样例\n",
    "\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import mnist_inference\n",
    "\n",
    "###################################################################################################\n",
    "# mnist 数据集相关常数\n",
    "# 输入层节点数。 对于mnist数据集，这个等于图片的像素\n",
    "INPUT_NODE = 784\n",
    "# 输出层的节点数。这个等于类别的数目\n",
    "OUTPUT_NODE = 10\n",
    "\n",
    "###################################################################################################\n",
    "# 配置神经网络参数\n",
    "# 隐藏层节点数\n",
    "LAYER1_NODE = 500\n",
    "# 一个训练batch中的训练数据个数， batch越大，训练越接近梯度下降；batch越小，训练越接近随机梯度下降\n",
    "BATCH_SIZE = 100\n",
    "# 基础学习率\n",
    "LEARNING_RATE_BASE = 0.8\n",
    "# 学习率衰减率\n",
    "LEARNING_RATE_DECAY = 0.99\n",
    "# 描述模型复杂度的正则化项在损失函数中的系数\n",
    "REGULARIZATION_RATE = 0.0001\n",
    "# 训练论数\n",
    "TRAINING_STEPS = 2000\n",
    "# 滑动平均衰减率\n",
    "MOVING_AVERAGE_DECAY = 0.99\n",
    "\n",
    "# 模型保存的路径和文件名。\n",
    "MODEL_SAVE_PATH = \"/data/logs/log_async\"\n",
    "# mnist数据路径\n",
    "DATA_PATH = \"/data/datasets/MNIST_data\"\n",
    "\n",
    "###################################################################################################\n",
    "# 通过flags指定运行的参数\n",
    "FLAGS = tf.app.flags.FLAGS\n",
    "# 指定当前运行的是参数服务器还是计算服务器\n",
    "# 参数服务器只负责TensorFlow中变量的维护和管理；计算服务器负责每一轮迭代时运行反向传播过程\n",
    "tf.app.flags.DEFINE_string(\"job_name\", 'worker', '\"ps\" or \"worker\"')\n",
    "\n",
    "# 指定集群中参数服务器地址\n",
    "tf.app.flags.DEFINE_string(\"ps_hosts\", 'tf-ps0:2222, tf-ps1:2223', 'Comma-separated list of hostname:port '\n",
    "                                        'for the parameter server jobs. e.g. \"tf-ps0:2222, tf-ps1:2223\"')\n",
    "\n",
    "# 指定集群中计算服务器地址\n",
    "tf.app.flags.DEFINE_string(('worker_hosts', 'tf-worker0:2222, tf-worker1:2223', 'Comma-separated list of hostname:port'\n",
    "                                         ' for the worker jobs. e.g. \"tf-worker0:2222, tf-worker1:2223\"'))\n",
    "\n",
    "# 指定当前任务ID\n",
    "# TensorFlow会自动根据参数服务器／计算服务器立标中国年的端口号来启动服务\n",
    "tf.app.flags.DEFINE_integer('task_id', 0, 'Task ID of the worker/replica running the training.')\n",
    "\n",
    "\n",
    "###################################################################################################\n",
    "# 定义TensorFlow的计算图并返回每一轮迭代需要运行的操作\n",
    "def buidl_model(x, y_, is_chief):\n",
    "\tregularizer = tf.contrib.layers.l2_regularizer(REGULARIZATION_RATE)\n",
    "\ty = mnist_inference.inference(x, regularizer)\n",
    "\tglobal_step = tf.Variable(0, trainable=False)\n",
    "\t\n",
    "\t# 定义损失函数并计算反向传播过程\n",
    "\tcross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(y, tf.argmax(y_, 1))\n",
    "\tcross_entropy_mean = tf.reduce_mean(cross_entropy)\n",
    "\tloss = cross_entropy_mean + tf.add_n(tf.get_collection('losses'))\n",
    "\tlearning_rate = tf.train.exponential_decay(LEARNING_RATE_BASE, global_step, 60000/BATCH_SIZE, LEARNING_RATE_DECAY)\n",
    "\t\n",
    "\t# 定义每一轮迭代需要的操作\n",
    "\ttrain_op = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "\t\n",
    "\t# 定义每一轮迭代需要运行的操作\n",
    "\tif is_chief:\n",
    "\t\t# 计算变量的滑动平均值\n",
    "\t\tvariable_averages = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY, global_step)\n",
    "\t\tvariable_averages_op = variable_averages.apply(tf.trainable_variables())\n",
    "\t\twith tf.control_dependencies(([variable_averages_op, train_op])):\n",
    "\t\t\ttrain_op = tf.no_op()\n",
    "\treturn global_step, loss, train_op\n",
    "\t\n",
    "\t\n",
    "###################################################################################################\n",
    "# 训练分布式深度学习模型的主要过程\n",
    "def main(argv=None):\n",
    "\t# 解析flags并通过tf.train.ClusterSpec配置TensorFlow集群\n",
    "\tps_hosts = FLAGS.ps_hosts.split(',')\n",
    "\tworker_hosts = FLAGS.worder_hosts.split(',')\n",
    "\tcluster = tf.train.ClusterSpec({'ps': ps_hosts, 'worker': worker_hosts})\n",
    "\t\n",
    "\t# 通过ClusterSpec以及当前任务创建Server\n",
    "\tserver = tf.train.Server(cluster, job_name=FLAGS.job_name, task_index=FLAGS.task_id)\n",
    "\t\n",
    "\t# 参数服务器只需要管理TensorFlow中的变量，不需要执行训练过程\n",
    "\t# server.join会一直定在这条语句上\n",
    "\tif FLAGS.job_name == 'ps':\n",
    "\t\tserver.join()\n",
    "\t\t\n",
    "\t# 定义计算服务器需要运行的操作\n",
    "\t# 在所有的计算服务器中有一个是主计算服务器，它除来负责计算反响传播的结果，还负责输出日志和保存模型\n",
    "\tis_chief = (FLAGS.task_id == 0)\n",
    "\tmnist = input_data.read_data_sets(DATA_PATH, one_hot=True)\n",
    "\t\n",
    "\t# 通过tf.train.replica_device_setter来指定执行每一个运算的设备\n",
    "\t# 此函数会自动将所有的参数分配到参数服务器上，而计算分配到当前的计算服务器上\n",
    "\twith tf.device(tf.train.replica_device_setter(worker_device=\"/job:worker/task:%d\" % FLAGS.task_id, cluster=cluster)):\n",
    "\t\tx = tf.placeholder(tf.float32, [None, mnist_inference.INPUT_NODE], name=\"x-input\")\n",
    "\t\ty_ = tf.placeholder(tf.float32, [None, mnist_inference.OUTPUT_NODE], name=\"y-input\")\n",
    "\t\t\n",
    "\t\t# 定义训练模型需要运行的操作\n",
    "\t\tglobal_step, loss, train_op = buidl_model(x, y_, is_chief)\n",
    "\t\t\n",
    "\t\t# 定义用于保存模型的saver\n",
    "\t\tsaver = tf.train.Saver()\n",
    "\t\t\n",
    "\t\t# 定义日志输出操作\n",
    "\t\tsummary_op = tf.merge_all_summaries()\n",
    "\t\t\n",
    "\t\t# 定义变量初始化操作\n",
    "\t\tinit_op = tf.initialize_all_variables()\n",
    "\t\t\n",
    "\t\t# 通过tf.train.Supervisor管理训练深度学习模型的通用功能\n",
    "\t\t# tf.train.Supervisor能统一管理队列操作、模型保存、日志输入、会话生成\n",
    "\t\tsv = tf.train.Supervisor(\n",
    "\t\t\tis_chief=is_chief,    # 定义当前计算服务器是否为主计算服务器，只有主计算服务器会保存模型和日志输出\n",
    "\t\t\tlogdir=MODEL_SAVE_PATH,   # 指定保存模型和输入日志的地址\n",
    "\t\t\tinit_op=init_op,    # 指定初始化操作\n",
    "\t\t\tsummary_op=summary_op,   # 指定日志生成操作\n",
    "\t\t\tsaver=saver,    # 指定用于保存模型的saver\n",
    "\t\t\tglobal_step=global_step,    # 指定昂欠迭代的轮数，这个会用于生成保存模型文件的文件名\n",
    "\t\t\tsave_model_secs=60,    # 指定保存模型的时间间隔\n",
    "\t\t\tsave_summaries_secs=60   # 指定日志输出的时间间隔\n",
    "\t\t)\n",
    "\t\t\n",
    "\t\tsess_config = tf.ConfigProto(allow_soft_placement=True, log_device_placement=True)\n",
    "\t\t\n",
    "\t\t# 通过tf.train.Supervisor生成会话\n",
    "\t\tsess = sv.prepare_or_wait_for_session(server.target, config=sess_config)\n",
    "\t\t\n",
    "\t\tstep = 0\n",
    "\t\tstart_time = time.time()\n",
    "\t\t\n",
    "\t\t# 执行迭代过程\n",
    "\t\t# 在迭代过程中tf.train.Supervisor会帮助输出日志和保存模型, 所以不需要直接调用这些过程\n",
    "\t\twhile not sv.should_stop():\n",
    "\t\t\txs, ys = mnist.train.next_batch(BATCH_SIZE)\n",
    "\t\t\t_, loss_value, global_step_value = sess.run([train_op, loss, global_step], feed_dict={x: xs, y_: ys})\n",
    "\t\t\tif global_step_value >= TRAINING_STEPS:\n",
    "\t\t\t\tbreak\n",
    "\t\t\t\t\n",
    "\t\t\t# 每隔一段时间输出训练信息\n",
    "\t\t\tif step > 0 and step % 100 == 0:\n",
    "\t\t\t\tduration = time.time() - start_time\n",
    "\t\t\t\n",
    "\t\t\t\t# 不同的训练服务器都会更新全局的训练轮数\n",
    "\t\t\t\t# 所以这里使用global_step_value可以直接得到在训练中使用过的batch总数\n",
    "\t\t\t\tsec_per_batch = duration / global_step_value\n",
    "\t\t\t\tformat_str = \"After %d training steps (%d global steps), loss on training batch is %g. (%.3f sec/batch)\"\n",
    "\t\t\t\tprint format_str % (step, global_step_value, loss_value, sec_per_batch)\n",
    "\t\t\t\t\n",
    "\t\t\t\tstep += 1\n",
    "\t\t\t\t\n",
    "\tsv.stop()\n",
    "\t\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\ttf.app.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
