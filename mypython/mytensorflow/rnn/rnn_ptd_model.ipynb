{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 使用rnn在ptb数据集上建立自然语言模型\n",
    "# ptb(Penn Treebank Dataset) 地址： http://www.fit.vutbr.cz/~imikolov/rnnlm/\n",
    "# wget http://www.fit.vutbr.cz/~imikolov/rnnlm/simple-examples.tgz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0 0 0]\n",
      " [0 1 0 0 0]\n",
      " [0 0 1 0 0]\n",
      " [0 0 0 1 0]\n",
      " [0 0 0 0 1]]\n",
      "=====\n",
      "[[0 1 0 0 0]\n",
      " [0 0 1 0 0]\n",
      " [0 0 0 1 0]\n",
      " [1 0 0 0 0]\n",
      " [0 0 0 1 0]\n",
      " [0 0 1 0 0]\n",
      " [0 1 0 0 0]]\n",
      "=====\n",
      "[[[0 1 0 0 0]\n",
      "  [0 0 1 0 0]]\n",
      "\n",
      " [[0 0 1 0 0]\n",
      "  [0 1 0 0 0]]\n",
      "\n",
      " [[0 0 0 1 0]\n",
      "  [0 0 0 1 0]]]\n"
     ]
    }
   ],
   "source": [
    "# tensorflow embedding_lookup 用法\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# 定义未知变量用于存储索引\n",
    "input_ids = tf.placeholder(dtype=tf.int32, shape=[None])\n",
    "input_ids2 = tf.placeholder(dtype=tf.int32, shape=[None, None])\n",
    "# 已知变量 5*5矩阵\n",
    "embedding = tf.Variable(np.identity(5), dtype=np.int32)\n",
    "\n",
    "# 根据input_ids索引，在embedding中寻找对应的元素\n",
    "input_embedding = tf.nn.embedding_lookup(embedding, input_ids)\n",
    "input_embedding2 = tf.nn.embedding_lookup(embedding, input_ids2)\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.initialize_all_variables())\n",
    "print embedding.eval()\n",
    "print '====='\n",
    "print(sess.run(input_embedding, feed_dict={input_ids:[1, 2, 3, 0, 3, 2, 1]}))\n",
    "print '====='\n",
    "print(sess.run(input_embedding2, feed_dict={input_ids2:[[1, 2], [2, 1], [3, 3]]}))\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  2  3]\n",
      " [ 4  5  6]\n",
      " [ 7  8  9]\n",
      " [10 11 12]]\n",
      "[[ 1  2  3  7  8  9]\n",
      " [ 4  5  6 10 11 12]]\n"
     ]
    }
   ],
   "source": [
    "# concat 使用\n",
    "# tf.concat(concat_dim, values, name='concat')\n",
    "# concat_dim是tensor连接的方向（维度），values是要连接的tensor链表，name是操作名\n",
    "# 两个二维tensor连接：concat_dim：0表示行，1表示列\n",
    "# 两个三维tensor连接concat_dim：0表示纵向，1表示行，2表示列\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "t1 = [[1,2,3], [4,5,6]]\n",
    "\n",
    "t2 = [[7,8,9], [10,11,12]]\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.initialize_all_variables())\n",
    "print sess.run(tf.concat(0, [t1, t2]))\n",
    "print sess.run(tf.concat(1, [t1, t2]))\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell.BasicLSTMCell object at 0x10b116750>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "929589\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell.BasicLSTMCell object at 0x10c71ff90>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In iteration: 1\n",
      "After 0 steps, perplexity is 10016.474\n",
      "After 100 steps, perplexity is 1480.141\n",
      "After 200 steps, perplexity is 1095.041\n",
      "After 300 steps, perplexity is 914.899\n",
      "After 400 steps, perplexity is 800.299\n",
      "After 500 steps, perplexity is 722.194\n",
      "After 600 steps, perplexity is 663.447\n",
      "After 700 steps, perplexity is 611.851\n",
      "After 800 steps, perplexity is 565.763\n",
      "After 900 steps, perplexity is 529.579\n",
      "After 1000 steps, perplexity is 501.825\n",
      "After 1100 steps, perplexity is 475.410\n",
      "After 1200 steps, perplexity is 453.699\n",
      "After 1300 steps, perplexity is 434.124\n",
      "Epoch: 1 validation perplexity: 243.041\n",
      "In iteration: 2\n",
      "After 0 steps, perplexity is 349.487\n",
      "After 100 steps, perplexity is 247.141\n",
      "After 200 steps, perplexity is 251.573\n",
      "After 300 steps, perplexity is 252.555\n",
      "After 400 steps, perplexity is 249.283\n",
      "After 500 steps, perplexity is 246.609\n",
      "After 600 steps, perplexity is 245.816\n",
      "After 700 steps, perplexity is 243.148\n",
      "After 800 steps, perplexity is 238.345\n",
      "After 900 steps, perplexity is 235.592\n",
      "After 1000 steps, perplexity is 233.871\n",
      "After 1100 steps, perplexity is 230.317\n",
      "After 1200 steps, perplexity is 227.609\n",
      "After 1300 steps, perplexity is 224.828\n",
      "Epoch: 2 validation perplexity: 185.159\n",
      "Test perplexity: 180.247\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/env/lib/python2.7/site-packages/IPython/core/interactiveshell.py:2889: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.models.rnn.ptb import reader\n",
    "\n",
    "# 常量定义\n",
    "HIDDEN_SIZE = 200   # 隐藏层大小\n",
    "NUM_LAYERS = 2      # 深层循环神经网络中的LSTM结构层数\n",
    "VOCAB_SIZE = 10000  # 词典规模\n",
    "\n",
    "LEARNING_RATE = 1.0    # 学习速率\n",
    "TRAIN_BATCH_SIZE = 20  # 训练数据batch大小\n",
    "TRAIN_NUM_STEP = 35    # 训练数据截断长度\n",
    "\n",
    "# 测试时不需要使用截断，所以可以将测试数据看成一个超长的序列\n",
    "EVAL_BATCH_SIZE = 1    # 测试数据batch大小\n",
    "EVAL_NUM_STEP = 1      # 测试数据截断长度\n",
    "NUM_EPOCH = 2          # 使用训练数据的轮数\n",
    "KEEP_PROB = 0.5        # 节点不被dropout的概率\n",
    "MAX_GRAD_NORM = 5      # 用于控制梯度膨胀的参数\n",
    "\n",
    "\n",
    "class PTBModel(object):\n",
    "    def __init__(self, is_training, batch_size, num_steps):\n",
    "        # 记录使用的batch大小和截断长度\n",
    "        self.batch_size = batch_size\n",
    "        self.num_steps = num_steps\n",
    "        \n",
    "        # 定义输入层\n",
    "        self.input_data = tf.placeholder(tf.int32, [batch_size, num_steps])\n",
    "        # 定义预期输出\n",
    "        self.targets = tf.placeholder(tf.int32, [batch_size, num_steps])\n",
    "        \n",
    "        # 定义使用LSTM结构为循环体结构，且使用dropout的深层循环神经网络\n",
    "        lstm_cell = tf.nn.rnn_cell.BasicLSTMCell(HIDDEN_SIZE)\n",
    "        if is_training:\n",
    "            lstm_cell = tf.nn.rnn_cell.DropoutWrapper(lstm_cell, output_keep_prob=KEEP_PROB)\n",
    "        cell = tf.nn.rnn_cell.MultiRNNCell([lstm_cell] * NUM_LAYERS)\n",
    "        \n",
    "        # 初始化最初的状态，也就是全零的向量\n",
    "        self.initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "        \n",
    "        ####################################################################################\n",
    "        # 将单词ID转换成单词向量。因为总共有VOCAB_SIZE个单词，每个单词向量的维度为HIDDEN_SIZE\n",
    "        # 所以embedding参数的维度为VOCAB_SIZE * HIDDEN_SIZE\n",
    "        embedding = tf.get_variable(\"embedding\", [VOCAB_SIZE, HIDDEN_SIZE])\n",
    "        # 将原本batch_size * num_steps 个单词ID转换为单词向量\n",
    "        # 转换后的输入层维度为batch_size * num_size * HIDDEN_SIZE\n",
    "        inputs = tf.nn.embedding_lookup(embedding, self.input_data)\n",
    "        # 只在训练的时候，使用dropout\n",
    "        if is_training:\n",
    "            inputs = tf.nn.dropout(inputs, KEEP_PROB)\n",
    "            \n",
    "        ####################################################################################\n",
    "        # 定义输出列表--先将不同时刻的LSTM结构输出收集起来，再通过一个全链接层得到最终的输出\n",
    "        outputs = []\n",
    "        # state 存储不同batch中的LSTM的状态，将其初始化为0\n",
    "        state = self.initial_state\n",
    "        with tf.variable_scope(\"RNN\"):\n",
    "            for time_step in range(num_steps):\n",
    "                if time_step > 0:\n",
    "                    tf.get_variable_scope().reuse_variables()\n",
    "                # 从输入数据中获取当前时刻的输入并传入LSTM结构\n",
    "                cell_output, state = cell(inputs[:, time_step, :], state)\n",
    "                # 将当前输出加入输出队列\n",
    "                outputs.append(cell_output)\n",
    "        # 把输出队列展开成[batch, hidden * num_steps]的形状，然后再reshape成[batch* num_steps, hidden_size]的形状\n",
    "        output = tf.reshape(tf.concat(1, outputs), [-1, HIDDEN_SIZE])\n",
    "        \n",
    "        # 从LSTM中得到的输出再经过一个全链接层得到的预测结果，最终的预测的结果在没一个时刻上都是长度为VOCAB_SIZE的数组\n",
    "        # 经过softmax层之后表示下一个位置是不同单词的概率\n",
    "        weight = tf.get_variable(\"weight\", [HIDDEN_SIZE, VOCAB_SIZE])\n",
    "        bias = tf.get_variable(\"bias\", [VOCAB_SIZE])\n",
    "        logits = tf.matmul(output, weight) + bias\n",
    "        \n",
    "        ####################################################################################\n",
    "        # 定义交叉上损失函数\n",
    "        # sequence_loss_by_example函数计算一个序列的交叉熵的和\n",
    "        loss = tf.nn.seq2seq.sequence_loss_by_example(\n",
    "            [logits],    # 预测的结果\n",
    "            [tf.reshape(self.targets, [-1])], # 期待的正确答案，这里将[batch_size, num_steps] 二维数字压缩成一维数组\n",
    "            # 损失权重。在这里所有的权重都为1，也就是不同batch和不同时刻的重要程度是一样的\n",
    "            [tf.ones([batch_size * num_steps], dtype=tf.float32)]  \n",
    "        )\n",
    "        # 计算每个batch的平均损失\n",
    "        self.cost = tf.reduce_sum(loss) / batch_size\n",
    "        self.final_state = state\n",
    "        \n",
    "        # 只在训练模型时定义反向传播操作\n",
    "        if not is_training:\n",
    "            return\n",
    "        ####################################################################################\n",
    "        trainable_variables = tf.trainable_variables()\n",
    "        \n",
    "        # Gradient clipping 是为了处理gradient explosion和gradient vanishing。直观作用是让权重的更新限制在一个合理的范围内\n",
    "        # 通过clip_by_global_norm函数控制梯度的大小，避免梯度膨胀问题\n",
    "        # t_list[i] * clip_norm / max(global_norm, clip_norm)\n",
    "        # global_norm = sqrt(sum([l2norm(t)**2 for t in t_list]))\n",
    "        grads, _ = tf.clip_by_global_norm(tf.gradients(self.cost, trainable_variables), MAX_GRAD_NORM)\n",
    "        # 定义优化方法\n",
    "        optimizer = tf.train.GradientDescentOptimizer(LEARNING_RATE)\n",
    "        # 定义训练步骤\n",
    "        self.train_op = optimizer.apply_gradients(zip(grads, trainable_variables))\n",
    "\n",
    "def run_epoch(sess, model, data, train_op, output_log):\n",
    "    # 使用给定的模型model，在数据data上运行train_op并返回在全部数据上的perplexity值\n",
    "    \n",
    "    # 计算perplexity的辅助变量\n",
    "    total_costs = 0.0\n",
    "    iters = 0\n",
    "    state = sess.run(model.initial_state)\n",
    "    \n",
    "    # 使用当前的数据训练或者测试模型\n",
    "    for step, (x, y) in enumerate(reader.ptb_iterator(data, model.batch_size, model.num_steps)):\n",
    "        # 在当前batch上运行train_op并计算损失值。交叉熵损失函数计算的就是下一个单词给定单词的概率\n",
    "        cost, state, _ = sess.run([model.cost, model.final_state, train_op],\n",
    "                                 {model.input_data: x, model.targets: y,\n",
    "                                 model.initial_state: state})\n",
    "        # 将不同时刻，不同batch的概率加起来可以得到第二个perplexity公式等号右边的部分，再将这个和做指数运算就可以得到perplexity\n",
    "        total_costs += cost\n",
    "        iters += model.num_steps\n",
    "        \n",
    "        # 只有在训练时输出日志\n",
    "        if output_log and step % 100 == 0:\n",
    "            print \"After %d steps, perplexity is %.3f\" % (step, np.exp(total_costs/ iters))\n",
    "    \n",
    "    # 返回指定模型在给定数据上的perplexity值\n",
    "    return np.exp(total_costs/iters)\n",
    "        \n",
    "\n",
    "def main(_):\n",
    "    # 获取原始数据\n",
    "    train_data, valid_data, test_data, _ = reader.ptb_raw_data(\"/Users/xxx/work5/tensorflow/data/ptb_dataset/simple-examples/data\")\n",
    "    print len(train_data)\n",
    "    \n",
    "    # 定义初始化函数\n",
    "    initializer = tf.random_uniform_initializer(-0.05, 0.05)\n",
    "    # 定义训练用的神经网络模型\n",
    "    with tf.variable_scope(\"language_model\", reuse=None, initializer=initializer):\n",
    "        train_model = PTBModel(True, TRAIN_BATCH_SIZE, TRAIN_NUM_STEP)\n",
    "    \n",
    "    # 定义评测用的神经网络模型\n",
    "    with tf.variable_scope(\"language_model\", reuse=True, initializer=initializer):\n",
    "        eval_model = PTBModel(False, EVAL_BATCH_SIZE, EVAL_NUM_STEP)\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        tf.initialize_all_variables().run()\n",
    "        \n",
    "        # 使用训练数据训练模型\n",
    "        for i in range(NUM_EPOCH):\n",
    "            print \"In iteration: %d\" % (i + 1)\n",
    "            # 在训练数据上训练神经网络模型\n",
    "            run_epoch(sess, train_model, train_data, train_model.train_op, True)\n",
    "            \n",
    "            # 使用验证数据评测模型效果\n",
    "            valid_perplexity = run_epoch(sess, eval_model, valid_data, tf.no_op(), False)\n",
    "            print \"Epoch: %d validation perplexity: %.3f\" % (i + 1, valid_perplexity)\n",
    "            \n",
    "        # 最后使用测试数据测试模型效果\n",
    "        test_perplexity = run_epoch(sess, eval_model, test_data, tf.no_op(), False)\n",
    "        \n",
    "        print \"Test perplexity: %.3f\" % test_perplexity\n",
    "    \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    tf.app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[2, 1]], dtype=int32)]\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
