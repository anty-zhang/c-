{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 深度学习并行模式\n",
    "\n",
    "在并行化地训练深度学习模型时，不同设备（GPU／CPU）可以在不同训练数据上运行这个迭代过程，而不同并行模式的区别在于不同的参数更新方式\n",
    "\n",
    "### 同步\n",
    "\n",
    "> 所有设备同时读取参数的取值，并且当反向传播算法完成之后，同步更新参数的取值\n",
    "\n",
    "### 异步\n",
    "\n",
    "> 在异步模式下，不同设备之间时完全独立的\n",
    "\n",
    "> 异步模式训练的深度学习模型有可能无法达到较优的训练结果\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# 多GPU并行\n",
    "# 一般来说一台机器上的多个GPU性能相似，所以这种设置下会更多的采用同步模式训练深度学习模型\n",
    "\n",
    "import tensorflow as tf\n",
    "import mnist_inference\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "###################################################################################################\n",
    "# 配置神经网络参数\n",
    "\n",
    "# 一个训练batch中的训练数据个数， batch越大，训练越接近梯度下降；batch越小，训练越接近随机梯度下降\n",
    "BATCH_SIZE = 100\n",
    "# 基础学习率\n",
    "LEARNING_RATE_BASE = 0.001\n",
    "# 学习率衰减率\n",
    "LEARNING_RATE_DECAY = 0.99\n",
    "# 描述模型复杂度的正则化项在损失函数中的系数\n",
    "REGULARIZATION_RATE = 0.0001\n",
    "# 训练论数\n",
    "TRAINING_STEPS = 1000\n",
    "# 滑动平均衰减率\n",
    "MOVING_AVERAGE_DECAY = 0.99\n",
    "#  gpu个数\n",
    "N_GPU = 4\n",
    "\n",
    "# 模型保存的路径和文件名\n",
    "MODEL_SAVE_PATH = \"/data/tf_out\"\n",
    "MODEL_NAME = \"model.ckpt\"\n",
    "\n",
    "# 定义数据存储的路径。\n",
    "# 因为需要为不同的GPU提供不同的训练数据，所以placeerholder的方式就需要手动准备多份数据，因此使用输入队列的方式从TFRecord中读取数据\n",
    "# 因此DATA_PATH为MNIST训练数据转化为TFRecord格式之后的路径\n",
    "DATA_PATH = \"/data/data.tfrecords\"\n",
    "\n",
    "###################################################################################################\n",
    "# 定义输入队列得到训练数据\n",
    "def get_input():\n",
    "    filename_queue = tf.train.string_input_producer([DATA_PATH])\n",
    "    reader = tf.TFRecordReader()\n",
    "    _, serialized_example = reader.read(filename_queue)\n",
    "    #  定义数据解析格式\n",
    "    features = tf.parse_single_example(\n",
    "        serialized_example,\n",
    "        features={\n",
    "            'image_raw': tf.FixedLenFeature([], tf.string),\n",
    "            'pixels': tf.FixedLenFeature([], tf.int64),\n",
    "            'label': tf.FixedLenFeature([], tf.int64)\n",
    "        })\n",
    "    \n",
    "    # 解析图片和标签信息\n",
    "    decoded_image = tf.decode_raw(features['image_raw'], tf.uint8)\n",
    "    reshaped_image = tf.reshape(decoded_image, [784])\n",
    "    retyped_image = tf.cast(reshaped_image, tf.float32)\n",
    "    label = tf.cast(features['label'], tf.int32)\n",
    "    \n",
    "    # 定时输入队列并返回\n",
    "    min_after_dequeue = 1000\n",
    "    capacity = min_after_dequeue + 3 * BATCH_SIZE\n",
    "    return tf.train.shuffle_batch(\n",
    "        [retyped_image, label],\n",
    "        batch_size=BATCH_SIZE,\n",
    "        capacity=capacity,\n",
    "        min_after_dequeue=min_after_dequeue)\n",
    "\n",
    "\n",
    "###################################################################################################\n",
    "# 定义损失函数\n",
    "# 对于给定的训练数据、正则化损失计算规则和命名空间，计算在这个命名空间下的总损失\n",
    "# 之所以需要给定命名空间是因为不同的GPU上计算得出的正则化损失都会加入名为loss的集合\n",
    "# 如果不通过命名空间就会将不同的GPU上的正则化损失全都加进来\n",
    "def get_loss(x, y_, regularizer, scope):\n",
    "    # 计算神经网络前向传播结果\n",
    "    y = mnist_inference.inference(x, regularizer)\n",
    "    # 计算交叉熵损失\n",
    "    cross_entropy = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(y, y_))\n",
    "    # 计算当前GPU上计算得到的正则化损失\n",
    "    regularization_loss = tf.add_n(tf.get_collection('losses', scope))\n",
    "    loss = cross_entropy + regularization_loss\n",
    "    return loss\n",
    "\n",
    "###################################################################################################\n",
    "# 计算每一个变量梯度的平均值\n",
    "def average_gradients(tower_grads):\n",
    "    average_grads_list = []\n",
    "    # 枚举所有的变量和变量在不同的GPU上计算得出的梯度\n",
    "    for grad_and_vars in zip(*tower_grads):\n",
    "        # 计算所有GPU上梯度的平均值\n",
    "        grads_list = []\n",
    "        for g, _ in grad_and_vars:\n",
    "            expanded_g = tf.expand_dims(g, 0)\n",
    "            grads_list.append(expanded_g)\n",
    "        grad = tf.concat(0, grads_list)\n",
    "        grad = tf.reduce_mean(grad, 0)\n",
    "        \n",
    "        v = grad_and_vars[0][1]\n",
    "        grad_and_var = (grad, v)\n",
    "        # 将变量和它的平均梯度对应起来\n",
    "        average_grads_list.append(grad_and_var)\n",
    "    # 返回所有变量的平均梯度，这将被用于变量更新\n",
    "    return average_grads_list\n",
    "\n",
    "###################################################################################################\n",
    "# 主训练过程\n",
    "def main(argv=None):\n",
    "    # 将简单的运算放在CPU上，只有神经网络的训练过程放在GPU上\n",
    "    with tf.Graph().as_default(), tf.device('/cpu:0'):\n",
    "        # 获取训练batch\n",
    "        x, y_ = get_input()\n",
    "        regularizer = tf.contrib.layers.l2_regularizer(REGULARIZATION_RATE)\n",
    "        \n",
    "        # 定义训练轮数和指数衰减的学习率\n",
    "        global_step = tf.get_variable('global_step', [], initializer=tf.constant_initializer(0), trainable=False)\n",
    "        learning_rate = tf.train.exponential_decay(LEARNING_RATE_BASE, global_step, \n",
    "                                                   60000/BATCH_SIZE, LEARNING_RATE_DECAY)\n",
    "        # 定义优化方法\n",
    "        opt = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "        \n",
    "        tower_grads_list = []\n",
    "        # 将神经网络的优化过程跑在不同的GPU上\n",
    "        for i in range(N_GPU):\n",
    "            # 将优化过程指定在一个GPU上\n",
    "            with tf.device('/gpu:%d' % i):\n",
    "                with tf.name_scope(\"GPU_%d\" % i) as scope:\n",
    "                    cur_loss = get_loss(x, y_, regularizer, scope)\n",
    "                    # 在第一次声明变量之后，将控制变量重用的参数设置为True，这样可以让不同的GPU更新同一组参数\n",
    "                    tf.get_variable_scope().reuse_variables()\n",
    "                    \n",
    "                    # 使用当前的GPU计算所有变量的梯度\n",
    "                    grads = opt.compute_gradients(cur_loss)\n",
    "                    tower_grads_list.append(grads)\n",
    "                    \n",
    "        # 计算变量的平均梯度，并输出到TensorBoard日志\n",
    "        grads = average_gradients(tower_grads_list)\n",
    "        for grad, var in grads:\n",
    "            if grad is not None:\n",
    "                tf.histogram_summary('gradients_on_average/%s' % var.op.name, grad)\n",
    "        \n",
    "        # 使用平均梯度更新参数\n",
    "        apply_gradient_op = opt.apply_gradients(grads, global_step=global_step)\n",
    "        for var in tf.trainable_variables():\n",
    "            tf.histogram_summary(var.op.name, var)\n",
    "            \n",
    "        # 计算变量的滑动平均值\n",
    "        variable_averages = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY, global_step)\n",
    "        variable_averages_op = variable_averages.apply(tf.trainable_variables())\n",
    "        \n",
    "        # 每一轮迭代需要更新变量的取值并更新变量的滑动平均值\n",
    "        train_op = tf.group(apply_gradient_op, variable_averages_op)\n",
    "        \n",
    "        saver = tf.train.Saver(tf.all_variables())\n",
    "        summary_op = tf.merge_all_summaries()\n",
    "        init = tf.initialize_all_variables()\n",
    "        \n",
    "        # 训练过程\n",
    "        with tf.Session(config=tf.ConfigProto(allow_soft_placement=True, log_device_placement=True)) as sess:\n",
    "            # 初始化所有变量并启动队列\n",
    "            init.run()\n",
    "            coord = tf.train.Coordinator()\n",
    "            threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "            summary_writer = tf.train.SummaryWriter(MODEL_SAVE_PATH, sess.graph)\n",
    "            \n",
    "            for step in range(TRAINING_STEPS):\n",
    "                # 执行神经网络训练操作，并记录训练操作的运行时间\n",
    "                start_time = time.time()\n",
    "                _, loss_value = sess.run([train_op, cur_loss])\n",
    "                duration = time.time() - start_time\n",
    "                \n",
    "                # 每隔一段时间展示当前的训练进度，并统计训练速度\n",
    "                if step != 0 adn step % 10 == 0:\n",
    "                    # 计算使用过的训练数据个数\n",
    "                    # 因为每一次运行训练操作时，每一个GPU都会使用一个batch的训练数据，所以总共用到的训练数据个数为batch * GPU个数\n",
    "                    num_example_per_step = BATCH_SIZE * N_GPU\n",
    "                    \n",
    "                    # 平均每秒可以处理的训练数据个数\n",
    "                    examples_per_sec = num_example_per_step / duration\n",
    "                    \n",
    "                    # 单个batch上的训练所需要的时间\n",
    "                    sec_per_batch = duration / N_GPU\n",
    "                    \n",
    "                    # 输出训练信息\n",
    "                    format_str = ('step %d, losss = %.2f (%1.f examples / sec; %.3f sec/batch)')\n",
    "                    print format_str % (step, loss_value, examples_per_sec, sec_per_batch)\n",
    "                    \n",
    "                    # 通过TensorBoard可视化训练过程\n",
    "                    summary = sess.run(summary_op)\n",
    "                    summary_writer.add_summary(summary, step)\n",
    "                \n",
    "                # 每隔一段时间保存当前的模型\n",
    "                if step % 1000 == 0 or (step + 1) == TRAINING_STEPS:\n",
    "                    checkpoint_path = os.path.join(MODEL_SAVE_PATH, MODEL_NAME)\n",
    "                    saver.save(sess, checkpoint_path, global_step=step)\n",
    "                    \n",
    "            coord.request_stop()\n",
    "            coord.join(threads)\n",
    "            \n",
    "            \n",
    "if __name__ == \"__main__\":\n",
    "    tf.app.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}