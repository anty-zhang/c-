{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mnist 数据集下载\n",
    "\n",
    "```sh\n",
    "# 地址\n",
    "http://yann.lecun.com/exdb/mnist/\n",
    "\n",
    "# download\n",
    "wget -c http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
    "wget -c http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
    "\n",
    "wget -c http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
    "\n",
    "wget -c http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /Users/xxx/work5/tensorflow/mnist/train-images-idx3-ubyte.gz\n",
      "Extracting /Users/xxx/work5/tensorflow/mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting /Users/xxx/work5/tensorflow/mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting /Users/xxx/work5/tensorflow/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Training data size:  55000\n",
      "Validating data size:  5000\n",
      "Testing data size:  10000\n",
      "Example training data:  [ 0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.38039219  0.37647063\n",
      "  0.3019608   0.46274513  0.2392157   0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.35294119  0.5411765\n",
      "  0.92156869  0.92156869  0.92156869  0.92156869  0.92156869  0.92156869\n",
      "  0.98431379  0.98431379  0.97254908  0.99607849  0.96078438  0.92156869\n",
      "  0.74509805  0.08235294  0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.54901963  0.98431379  0.99607849  0.99607849  0.99607849  0.99607849\n",
      "  0.99607849  0.99607849  0.99607849  0.99607849  0.99607849  0.99607849\n",
      "  0.99607849  0.99607849  0.99607849  0.99607849  0.74117649  0.09019608\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.88627458  0.99607849  0.81568635\n",
      "  0.78039223  0.78039223  0.78039223  0.78039223  0.54509807  0.2392157\n",
      "  0.2392157   0.2392157   0.2392157   0.2392157   0.50196081  0.8705883\n",
      "  0.99607849  0.99607849  0.74117649  0.08235294  0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.14901961  0.32156864  0.0509804   0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.13333334  0.83529419  0.99607849  0.99607849  0.45098042  0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.32941177  0.99607849  0.99607849  0.91764712  0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.32941177  0.99607849  0.99607849  0.91764712  0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.41568631  0.6156863   0.99607849  0.99607849  0.95294124  0.20000002\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.09803922  0.45882356  0.89411771\n",
      "  0.89411771  0.89411771  0.99215692  0.99607849  0.99607849  0.99607849\n",
      "  0.99607849  0.94117653  0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.26666668  0.4666667   0.86274517\n",
      "  0.99607849  0.99607849  0.99607849  0.99607849  0.99607849  0.99607849\n",
      "  0.99607849  0.99607849  0.99607849  0.55686277  0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.14509805  0.73333335  0.99215692\n",
      "  0.99607849  0.99607849  0.99607849  0.87450987  0.80784321  0.80784321\n",
      "  0.29411766  0.26666668  0.84313732  0.99607849  0.99607849  0.45882356\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.44313729\n",
      "  0.8588236   0.99607849  0.94901967  0.89019614  0.45098042  0.34901962\n",
      "  0.12156864  0.          0.          0.          0.          0.7843138\n",
      "  0.99607849  0.9450981   0.16078432  0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.66274512  0.99607849  0.6901961   0.24313727  0.          0.\n",
      "  0.          0.          0.          0.          0.          0.18823531\n",
      "  0.90588242  0.99607849  0.91764712  0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.07058824  0.48627454  0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.32941177  0.99607849  0.99607849  0.65098041  0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.54509807  0.99607849  0.9333334   0.22352943  0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.82352948  0.98039222  0.99607849  0.65882355  0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.94901967  0.99607849  0.93725497  0.22352943  0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.34901962  0.98431379  0.9450981   0.33725491  0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.01960784  0.80784321  0.96470594  0.6156863   0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.01568628  0.45882356  0.27058825  0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.        ]\n",
      "Example training data label:  [ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "###################################################################################################\n",
    "# tensorflow 对mnist数据集的封装\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "# 从path中载入数据集\n",
    "mnist = input_data.read_data_sets(\"/Users/xxx/work5/tensorflow/mnist\",one_hot=True)\n",
    "\n",
    "# training data size\n",
    "print \"Training data size: \", mnist.train.num_examples\n",
    "print \"Validating data size: \", mnist.validation.num_examples\n",
    "print \"Testing data size: \", mnist.test.num_examples\n",
    "print \"Example training data: \", mnist.train.images[0]\n",
    "print \"Example training data label: \", mnist.train.labels[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 神经网络实例（全模型）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /Users/xxx/work5/tensorflow/mnist/train-images-idx3-ubyte.gz\n",
      "Extracting /Users/xxx/work5/tensorflow/mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting /Users/xxx/work5/tensorflow/mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting /Users/xxx/work5/tensorflow/mnist/t10k-labels-idx1-ubyte.gz\n",
      "After 0 training step, validation accuracy using average model is: 0.059\n",
      "After 1000 training step, validation accuracy using average model is: 0.9774\n",
      "After 2000 training step, validation accuracy using average model is: 0.9824\n",
      "After 3000 training step, validation accuracy using average model is: 0.9842\n",
      "After 4000 training step, validation accuracy using average model is: 0.9858\n",
      "After 5000 training step, validation accuracy using average model is: 0.9846\n",
      "After 6000 training step, validation accuracy using average model is: 0.9848\n",
      "After 7000 training step, validation accuracy using average model is: 0.985\n",
      "After 8000 training step, validation accuracy using average model is: 0.9854\n",
      "After 9000 training step, validation accuracy using average model is: 0.9858\n",
      "After 10000 training step, validation accuracy using average model is: 0.9854\n",
      "After 11000 training step, validation accuracy using average model is: 0.9862\n",
      "After 12000 training step, validation accuracy using average model is: 0.9856\n",
      "After 13000 training step, validation accuracy using average model is: 0.985\n",
      "After 14000 training step, validation accuracy using average model is: 0.9856\n",
      "After 15000 training step, validation accuracy using average model is: 0.9854\n",
      "After 16000 training step, validation accuracy using average model is: 0.9848\n",
      "After 17000 training step, validation accuracy using average model is: 0.986\n",
      "After 18000 training step, validation accuracy using average model is: 0.9854\n",
      "After 19000 training step, validation accuracy using average model is: 0.9852\n",
      "After 20000 training step, validation accuracy using average model is: 0.9848\n",
      "After 21000 training step, validation accuracy using average model is: 0.9856\n",
      "After 22000 training step, validation accuracy using average model is: 0.9852\n",
      "After 23000 training step, validation accuracy using average model is: 0.986\n",
      "After 24000 training step, validation accuracy using average model is: 0.9862\n",
      "After 25000 training step, validation accuracy using average model is: 0.9856\n",
      "After 26000 training step, validation accuracy using average model is: 0.9848\n",
      "After 27000 training step, validation accuracy using average model is: 0.9856\n",
      "After 28000 training step, validation accuracy using average model is: 0.9854\n",
      "After 29000 training step, validation accuracy using average model is: 0.9856\n",
      "After 30000 training step, test accurary using average model is: 0.9842\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "###################################################################################################\n",
    "# 神经网络实例1\n",
    "# tensorflow 使用mnist训练神经网络实例\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "###################################################################################################\n",
    "# mnist 数据集相关常数\n",
    "# 输入层节点数。 对于mnist数据集，这个等于图片的像素\n",
    "INPUT_NODE = 784\n",
    "# 输出层的节点数。这个等于类别的数目\n",
    "OUTPUT_NODE = 10\n",
    "\n",
    "###################################################################################################\n",
    "# 配置神经网络参数\n",
    "# 隐藏层节点数\n",
    "LAYER1_NODE = 500\n",
    "# 一个训练batch中的训练数据个数， batch越大，训练越接近梯度下降；batch越小，训练越接近随机梯度下降\n",
    "BATCH_SIZE = 100\n",
    "# 基础学习率\n",
    "LEARNING_RATE_BASE = 0.8\n",
    "# 学习率衰减率\n",
    "LEARNING_RATE_DECAY = 0.99\n",
    "# 描述模型复杂度的正则化项在损失函数中的系数\n",
    "REGULARIZATION_RATE = 0.0001\n",
    "# 训练论数\n",
    "TRAINING_STEPS = 30000\n",
    "# 滑动平均衰减率\n",
    "MOVING_AVERAGE_DECAY = 0.99\n",
    "\n",
    "###################################################################################################\n",
    "# 辅助函数，给定了神经网络的输入和所有参数，计算神经网络前向传播结果\n",
    "# 使用relu激活函数实现了去线性化， 实现了三层全连接神经网络\n",
    "# 在函数中支持了传入计算参数平均值的类，可在测试时使用滑动平均模型\n",
    "def inference(input_tensor, avg_class, weights1, biases1, weights2, biases2):\n",
    "    # 当没有提供滑动平均类时，直接使用参数当前的取值\n",
    "    if avg_class is None:\n",
    "        # 计算隐藏层的前向传播结果，并且使用relu激活函数\n",
    "        layer1 = tf.nn.relu(tf.matmul(input_tensor, weights1) + biases1)\n",
    "        # 计算输出层的前向传播结果\n",
    "        # 输出层不加入激活函数原因： 在计算损失函数时会一并计算softmax函数\n",
    "        # 不加入softmax不会影响预测结果原因：预测时使用的是不同类别对应节点输出值的相对大小，所以没有softmax层对最后分类结果\n",
    "        # 的计算没有影响。于是在计算整个神经网络的前向传播时可以不加入最后的softmax层\n",
    "        return tf.matmul(layer1, weights2) + biases2\n",
    "    else:\n",
    "        # 首先通过avg_class.average函数计算得出变量的滑动平均值，然后再计算神经网络的前向传播结果\n",
    "        layer1 = tf.nn.relu(tf.matmul(input_tensor, avg_class.average(weights1)) + avg_class.average(biases1))\n",
    "        return tf.matmul(layer1, avg_class.average(weights2)) + avg_class.average(biases2)\n",
    "\n",
    "\n",
    "###################################################################################################\n",
    "# 训练模型\n",
    "def train(mnist):\n",
    "    x = tf.placeholder(tf.float32, [None, INPUT_NODE], name=\"x-input\")\n",
    "    y_ = tf.placeholder(tf.float32, [None, OUTPUT_NODE], name=\"y-input\")\n",
    "    \n",
    "    # 生成隐藏层参数\n",
    "    weights1 = tf.Variable(tf.truncated_normal([INPUT_NODE, LAYER1_NODE], stddev=0.1))\n",
    "    biases1 = tf.Variable(tf.constant(0.1, shape=[LAYER1_NODE]))\n",
    "    # 生成输出层参数\n",
    "    weights2 = tf.Variable(tf.truncated_normal([LAYER1_NODE, OUTPUT_NODE], stddev=0.1))\n",
    "    biases2 = tf.Variable(tf.constant(0.1, shape=[OUTPUT_NODE]))\n",
    "    \n",
    "    # 计算当前参数下神经网络的前向传播结果\n",
    "    y = inference(x, None, weights1, biases1, weights2, biases2)\n",
    "    \n",
    "    # 定义存储训练轮数的变量\n",
    "    # 此变量不需要计算滑动平均值，所以这里指定这个变量为不可训练的变量\n",
    "    # 在tensorflow训练神经网络时，一般会将嗲表训练轮数的变量指定为不可训练的变量\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "    \n",
    "    # --------------------------------------------------------------------------------------------\n",
    "    # 给定滑动平均衰减率和训练轮数的变量，初始化滑动平均类；给定训练轮数的变量可以加快训练早期变量的更新速度\n",
    "    variable_averages = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY, global_step)\n",
    "    # 在所有代表神经网络参数的变量上使用滑动平均（辅助变量不需要）\n",
    "    # tf.trainable_variables返回的就是图上集合GraphKeys.TRAINABLE_AVERAGES中的元素。这个集合的元素就是所有没有指定\n",
    "    # trainable=False的参数\n",
    "    variable_averages_op = variable_averages.apply(tf.trainable_variables())\n",
    "    # 计算使用了滑动平均之后的前向传播结果\n",
    "    average_y = inference(x, variable_averages, weights1, biases1, weights2, biases2)\n",
    "    \n",
    "    # --------------------------------------------------------------------------------------------\n",
    "    # 计算交叉熵作为刻画预测值和真实值之间差距的损失函数\n",
    "    # 当分类问题只有一个正确答案时，可以使用sparse_softmax_cross_entropy_with_logits函数来加速交叉熵计算\n",
    "    # tf.argmax函数得到正确答案对应的类别编号\n",
    "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(y, tf.argmax(y_, 1))\n",
    "    # 计算在当前batch中所有样例的交叉熵平均值\n",
    "    cross_entropy_mean = tf.reduce_mean(cross_entropy)\n",
    "    \n",
    "    # 计算L2正则化损失函数\n",
    "    regularizer = tf.contrib.layers.l2_regularizer(REGULARIZATION_RATE)\n",
    "    # 计算模型的正则化损失。 一般只计算神经网络边上权重的正则化损失，而不使用偏置项\n",
    "    regularization = regularizer(weights1) + regularizer(weights2)\n",
    "    \n",
    "    # 总损失等于交叉熵损失和正则化损失之和\n",
    "    loss = cross_entropy_mean + regularization\n",
    "    \n",
    "    # --------------------------------------------------------------------------------------------\n",
    "    # 设置指数衰减的学习率\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        LEARNING_RATE_BASE,    # 基础学习率，随着迭代的进行，更新变量时使用的学习率在这个基础上递减 \n",
    "        global_step,           # 当前迭代的轮数\n",
    "        mnist.train.num_examples/BATCH_SIZE,    # 总的迭代次数\n",
    "        LEARNING_RATE_DECAY)                    # 学习率递减速度\n",
    "    # 使用tf.train.GradientDescentOptimizer优化算法来优化损失函数\n",
    "    # 这里的损失函数包括了交叉熵损失和L2正则化损失\n",
    "    train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "    \n",
    "    # --------------------------------------------------------------------------------------------\n",
    "    # 在训练神经网络模型时，每过一遍数据既要通过反向传播算法来更新神经网络中的参数，又要更新每一个参数的滑动平均值\n",
    "    # 这一过程，tensorflow提供了tf.control_dependecies和tf.group两种机制\n",
    "    # train_op = tf.group(train_step, variable_averages_op)\n",
    "    with tf.control_dependencies([train_step, variable_averages_op]):\n",
    "        train_op = tf.no_op(name='train')\n",
    "        \n",
    "    # --------------------------------------------------------------------------------------------\n",
    "    # 检验使用了滑动平均模型的神经网络前向传播结果是否正确\n",
    "    # tf.equal 判断两个张量的每一维是否相等，相等返回True，否则返回False\n",
    "    correct_prediction = tf.equal(tf.argmax(average_y, 1), tf.argmax(y_, 1))\n",
    "    # 将bool的数值转为实数型，然后计算平均值。这个平均值就是模型在这一组数据上的正确率\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "    # --------------------------------------------------------------------------------------------\n",
    "    # 初始化会话并开始训练过程\n",
    "    with tf.Session() as sess:\n",
    "        tf.initialize_all_variables().run()\n",
    "        # 准备验证数据。一般在神经网络的训练过程中会通过验证数据来大致判断停止的条件和判断训练的效果\n",
    "        validate_feed = {x: mnist.validation.images, y_: mnist.validation.labels}\n",
    "        # 准备测试数据。在真实的应用中，这部分数据在训练时是不可见的，这个数据只是作为模型优劣的最后评价标准\n",
    "        test_feed = {x: mnist.test.images, y_: mnist.test.labels}\n",
    "        \n",
    "        # 迭代的训练神经网络\n",
    "        for i in range(TRAINING_STEPS):\n",
    "            # 每1000轮输出一次在验证数据集上的测试结果\n",
    "            if i % 1000 == 0:\n",
    "                # 计算滑动平均模型在验证数据上的结果。\n",
    "                # 由于mnist的数据集比较小，所以一次可以处理所有的验证数据。\n",
    "                # 在神经网络比较复杂或者验证数据比较大时，可以将验证数据划分为更新的batch，以免发生内存溢出错误\n",
    "                validate_acc = sess.run(accuracy, feed_dict=validate_feed)\n",
    "                print(\"After %d training step, validation accuracy using average model is: %g\" % (i, validate_acc))\n",
    "            \n",
    "            # 产生这一轮使用的一个batch的训练数据，并运行训练过程\n",
    "            xs, ys = mnist.train.next_batch(BATCH_SIZE)\n",
    "            sess.run(train_op, feed_dict={x: xs, y_: ys})\n",
    "        \n",
    "        # 在训练结束之后，在测试数据上检测神经网络模型的最终正确率\n",
    "        test_acc = sess.run(accuracy, feed_dict=test_feed)\n",
    "        print (\"After %d training step, test accurary using average model is: %g\" % (TRAINING_STEPS, test_acc))\n",
    "        \n",
    "###################################################################################################\n",
    "# 主程序入口\n",
    "def main(argv=None):\n",
    "    mnist = input_data.read_data_sets(\"/Users/xxx/work5/tensorflow/mnist\", one_hot=True)\n",
    "    train(mnist)\n",
    "\n",
    "###################################################################################################\n",
    "# tesorflow提供了一个主程序的入口，tf.app.run会调用上面定义的main函数\n",
    "if __name__ == '__main__':\n",
    "    tf.app.run()\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 神经网络实例---不使用激活函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /Users/xxx/work5/tensorflow/mnist/train-images-idx3-ubyte.gz\n",
      "Extracting /Users/xxx/work5/tensorflow/mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting /Users/xxx/work5/tensorflow/mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting /Users/xxx/work5/tensorflow/mnist/t10k-labels-idx1-ubyte.gz\n",
      "After 0 training step, validation accuracy using average model is: 0.0956\n",
      "After 1000 training step, validation accuracy using average model is: 0.0958\n",
      "After 2000 training step, validation accuracy using average model is: 0.0958\n",
      "After 3000 training step, validation accuracy using average model is: 0.0958\n",
      "After 4000 training step, validation accuracy using average model is: 0.0958\n",
      "After 5000 training step, test accurary using average model is: 0.098\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "###################################################################################################\n",
    "# 神经网络实例---不适用激活函数\n",
    "# tensorflow 使用mnist训练神经网络实例\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "###################################################################################################\n",
    "# mnist 数据集相关常数\n",
    "# 输入层节点数。 对于mnist数据集，这个等于图片的像素\n",
    "INPUT_NODE = 784\n",
    "# 输出层的节点数。这个等于类别的数目\n",
    "OUTPUT_NODE = 10\n",
    "\n",
    "###################################################################################################\n",
    "# 配置神经网络参数\n",
    "# 隐藏层节点数\n",
    "LAYER1_NODE = 500\n",
    "# 一个训练batch中的训练数据个数， batch越大，训练越接近梯度下降；batch越小，训练越接近随机梯度下降\n",
    "BATCH_SIZE = 100\n",
    "# 基础学习率\n",
    "LEARNING_RATE_BASE = 0.8\n",
    "# 学习率衰减率\n",
    "LEARNING_RATE_DECAY = 0.99\n",
    "# 描述模型复杂度的正则化项在损失函数中的系数\n",
    "REGULARIZATION_RATE = 0.0001\n",
    "# 训练论数\n",
    "TRAINING_STEPS = 5000\n",
    "# 滑动平均衰减率\n",
    "MOVING_AVERAGE_DECAY = 0.99\n",
    "\n",
    "###################################################################################################\n",
    "# 辅助函数，给定了神经网络的输入和所有参数，计算神经网络前向传播结果\n",
    "# 使用relu激活函数实现了去线性化， 实现了三层全连接神经网络\n",
    "# 在函数中支持了传入计算参数平均值的类，可在测试时使用滑动平均模型\n",
    "def inference(input_tensor, avg_class, weights1, biases1, weights2, biases2):\n",
    "    # 当没有提供滑动平均类时，直接使用参数当前的取值\n",
    "    if avg_class is None:\n",
    "        # 计算隐藏层的前向传播结果，并且使用relu激活函数\n",
    "        layer1 = tf.matmul(input_tensor, weights1) + biases1\n",
    "        # 计算输出层的前向传播结果\n",
    "        # 输出层不加入激活函数原因： 在计算损失函数时会一并计算softmax函数\n",
    "        # 不加入softmax不会影响预测结果原因：预测时使用的是不同类别对应节点输出值的相对大小，所以没有softmax层对最后分类结果\n",
    "        # 的计算没有影响。于是在计算整个神经网络的前向传播时可以不加入最后的softmax层\n",
    "        return tf.matmul(layer1, weights2) + biases2\n",
    "    else:\n",
    "        # 首先通过avg_class.average函数计算得出变量的滑动平均值，然后再计算神经网络的前向传播结果\n",
    "        layer1 = tf.matmul(input_tensor, avg_class.average(weights1)) + avg_class.average(biases1)\n",
    "        return tf.matmul(layer1, avg_class.average(weights2)) + avg_class.average(biases2)\n",
    "\n",
    "\n",
    "###################################################################################################\n",
    "# 训练模型\n",
    "def train(mnist):\n",
    "    x = tf.placeholder(tf.float32, [None, INPUT_NODE], name=\"x-input\")\n",
    "    y_ = tf.placeholder(tf.float32, [None, OUTPUT_NODE], name=\"y-input\")\n",
    "    \n",
    "    # 生成隐藏层参数\n",
    "    weights1 = tf.Variable(tf.truncated_normal([INPUT_NODE, LAYER1_NODE], stddev=0.1))\n",
    "    biases1 = tf.Variable(tf.constant(0.1, shape=[LAYER1_NODE]))\n",
    "    # 生成输出层参数\n",
    "    weights2 = tf.Variable(tf.truncated_normal([LAYER1_NODE, OUTPUT_NODE], stddev=0.1))\n",
    "    biases2 = tf.Variable(tf.constant(0.1, shape=[OUTPUT_NODE]))\n",
    "    \n",
    "    # 计算当前参数下神经网络的前向传播结果\n",
    "    y = inference(x, None, weights1, biases1, weights2, biases2)\n",
    "    \n",
    "    # 定义存储训练轮数的变量\n",
    "    # 此变量不需要计算滑动平均值，所以这里指定这个变量为不可训练的变量\n",
    "    # 在tensorflow训练神经网络时，一般会将嗲表训练轮数的变量指定为不可训练的变量\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "    \n",
    "    # --------------------------------------------------------------------------------------------\n",
    "    # 给定滑动平均衰减率和训练轮数的变量，初始化滑动平均类；给定训练轮数的变量可以加快训练早期变量的更新速度\n",
    "    variable_averages = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY, global_step)\n",
    "    # 在所有代表神经网络参数的变量上使用滑动平均（辅助变量不需要）\n",
    "    # tf.trainable_variables返回的就是图上集合GraphKeys.TRAINABLE_AVERAGES中的元素。这个集合的元素就是所有没有指定\n",
    "    # trainable=False的参数\n",
    "    variable_averages_op = variable_averages.apply(tf.trainable_variables())\n",
    "    # 计算使用了滑动平均之后的前向传播结果\n",
    "    average_y = inference(x, variable_averages, weights1, biases1, weights2, biases2)\n",
    "    \n",
    "    # --------------------------------------------------------------------------------------------\n",
    "    # 计算交叉熵作为刻画预测值和真实值之间差距的损失函数\n",
    "    # 当分类问题只有一个正确答案时，可以使用sparse_softmax_cross_entropy_with_logits函数来加速交叉熵计算\n",
    "    # tf.argmax函数得到正确答案对应的类别编号\n",
    "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(y, tf.argmax(y_, 1))\n",
    "    # 计算在当前batch中所有样例的交叉熵平均值\n",
    "    cross_entropy_mean = tf.reduce_mean(cross_entropy)\n",
    "    \n",
    "    # 计算L2正则化损失函数\n",
    "    regularizer = tf.contrib.layers.l2_regularizer(REGULARIZATION_RATE)\n",
    "    # 计算模型的正则化损失。 一般只计算神经网络边上权重的正则化损失，而不使用偏置项\n",
    "    regularization = regularizer(weights1) + regularizer(weights2)\n",
    "    \n",
    "    # 总损失等于交叉熵损失和正则化损失之和\n",
    "    loss = cross_entropy_mean + regularization\n",
    "    \n",
    "    # --------------------------------------------------------------------------------------------\n",
    "    # 设置指数衰减的学习率\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        LEARNING_RATE_BASE,    # 基础学习率，随着迭代的进行，更新变量时使用的学习率在这个基础上递减 \n",
    "        global_step,           # 当前迭代的轮数\n",
    "        mnist.train.num_examples/BATCH_SIZE,    # 总的迭代次数\n",
    "        LEARNING_RATE_DECAY)                    # 学习率递减速度\n",
    "    # 使用tf.train.GradientDescentOptimizer优化算法来优化损失函数\n",
    "    # 这里的损失函数包括了交叉熵损失和L2正则化损失\n",
    "    train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "    \n",
    "    # --------------------------------------------------------------------------------------------\n",
    "    # 在训练神经网络模型时，每过一遍数据既要通过反向传播算法来更新神经网络中的参数，又要更新每一个参数的滑动平均值\n",
    "    # 这一过程，tensorflow提供了tf.control_dependecies和tf.group两种机制\n",
    "    # train_op = tf.group(train_step, variable_averages_op)\n",
    "    with tf.control_dependencies([train_step, variable_averages_op]):\n",
    "        train_op = tf.no_op(name='train')\n",
    "        \n",
    "    # --------------------------------------------------------------------------------------------\n",
    "    # 检验使用了滑动平均模型的神经网络前向传播结果是否正确\n",
    "    # tf.equal 判断两个张量的每一维是否相等，相等返回True，否则返回False\n",
    "    correct_prediction = tf.equal(tf.argmax(average_y, 1), tf.argmax(y_, 1))\n",
    "    # 将bool的数值转为实数型，然后计算平均值。这个平均值就是模型在这一组数据上的正确率\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "    # --------------------------------------------------------------------------------------------\n",
    "    # 初始化会话并开始训练过程\n",
    "    with tf.Session() as sess:\n",
    "        tf.initialize_all_variables().run()\n",
    "        # 准备验证数据。一般在神经网络的训练过程中会通过验证数据来大致判断停止的条件和判断训练的效果\n",
    "        validate_feed = {x: mnist.validation.images, y_: mnist.validation.labels}\n",
    "        # 准备测试数据。在真实的应用中，这部分数据在训练时是不可见的，这个数据只是作为模型优劣的最后评价标准\n",
    "        test_feed = {x: mnist.test.images, y_: mnist.test.labels}\n",
    "        \n",
    "        # 迭代的训练神经网络\n",
    "        for i in range(TRAINING_STEPS):\n",
    "            # 每1000轮输出一次在验证数据集上的测试结果\n",
    "            if i % 1000 == 0:\n",
    "                # 计算滑动平均模型在验证数据上的结果。\n",
    "                # 由于mnist的数据集比较小，所以一次可以处理所有的验证数据。\n",
    "                # 在神经网络比较复杂或者验证数据比较大时，可以将验证数据划分为更新的batch，以免发生内存溢出错误\n",
    "                validate_acc = sess.run(accuracy, feed_dict=validate_feed)\n",
    "                print(\"After %d training step, validation accuracy using average model is: %g\" % (i, validate_acc))\n",
    "            \n",
    "            # 产生这一轮使用的一个batch的训练数据，并运行训练过程\n",
    "            xs, ys = mnist.train.next_batch(BATCH_SIZE)\n",
    "            sess.run(train_op, feed_dict={x: xs, y_: ys})\n",
    "        \n",
    "        # 在训练结束之后，在测试数据上检测神经网络模型的最终正确率\n",
    "        test_acc = sess.run(accuracy, feed_dict=test_feed)\n",
    "        print (\"After %d training step, test accurary using average model is: %g\" % (TRAINING_STEPS, test_acc))\n",
    "        \n",
    "###################################################################################################\n",
    "# 主程序入口\n",
    "def main(argv=None):\n",
    "    mnist = input_data.read_data_sets(\"/Users/xxx/work5/tensorflow/mnist\", one_hot=True)\n",
    "    train(mnist)\n",
    "\n",
    "###################################################################################################\n",
    "# tesorflow提供了一个主程序的入口，tf.app.run会调用上面定义的main函数\n",
    "if __name__ == '__main__':\n",
    "    tf.app.run()\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 神经网络实例---不使用隐藏层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /Users/xxx/work5/tensorflow/mnist/train-images-idx3-ubyte.gz\n",
      "Extracting /Users/xxx/work5/tensorflow/mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting /Users/xxx/work5/tensorflow/mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting /Users/xxx/work5/tensorflow/mnist/t10k-labels-idx1-ubyte.gz\n",
      "After 0 training step, validation accuracy using average model is: 0.0002\n",
      "After 1000 training step, validation accuracy using average model is: 0.9234\n",
      "After 2000 training step, validation accuracy using average model is: 0.9272\n",
      "After 3000 training step, validation accuracy using average model is: 0.9262\n",
      "After 4000 training step, validation accuracy using average model is: 0.928\n",
      "After 5000 training step, test accurary using average model is: 0.9253\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "###################################################################################################\n",
    "# 神经网络实例---不适用隐藏层\n",
    "# tensorflow 使用mnist训练神经网络实例\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "###################################################################################################\n",
    "# mnist 数据集相关常数\n",
    "# 输入层节点数。 对于mnist数据集，这个等于图片的像素\n",
    "INPUT_NODE = 784\n",
    "# 输出层的节点数。这个等于类别的数目\n",
    "OUTPUT_NODE = 10\n",
    "\n",
    "###################################################################################################\n",
    "# 配置神经网络参数\n",
    "# 隐藏层节点数\n",
    "LAYER1_NODE = 500\n",
    "# 一个训练batch中的训练数据个数， batch越大，训练越接近梯度下降；batch越小，训练越接近随机梯度下降\n",
    "BATCH_SIZE = 100\n",
    "# 基础学习率\n",
    "LEARNING_RATE_BASE = 0.8\n",
    "# 学习率衰减率\n",
    "LEARNING_RATE_DECAY = 0.99\n",
    "# 描述模型复杂度的正则化项在损失函数中的系数\n",
    "REGULARIZATION_RATE = 0.0001\n",
    "# 训练论数\n",
    "TRAINING_STEPS = 5000\n",
    "# 滑动平均衰减率\n",
    "MOVING_AVERAGE_DECAY = 0.99\n",
    "\n",
    "###################################################################################################\n",
    "# 辅助函数，给定了神经网络的输入和所有参数，计算神经网络前向传播结果\n",
    "# 使用relu激活函数实现了去线性化， 实现了三层全连接神经网络\n",
    "# 在函数中支持了传入计算参数平均值的类，可在测试时使用滑动平均模型\n",
    "def inference(input_tensor, avg_class, weights1, biases1):\n",
    "    # 当没有提供滑动平均类时，直接使用参数当前的取值\n",
    "    if avg_class is None:\n",
    "        # 计算隐藏层的前向传播结果，并且使用relu激活函数\n",
    "        layer1 = tf.nn.relu(tf.matmul(input_tensor, weights1) + biases1)\n",
    "        return layer1\n",
    "    else:\n",
    "        # 首先通过avg_class.average函数计算得出变量的滑动平均值，然后再计算神经网络的前向传播结果\n",
    "        layer1 = tf.nn.relu(tf.matmul(input_tensor, avg_class.average(weights1)) + avg_class.average(biases1))\n",
    "        return layer1\n",
    "\n",
    "\n",
    "###################################################################################################\n",
    "# 训练模型\n",
    "def train(mnist):\n",
    "    x = tf.placeholder(tf.float32, [None, INPUT_NODE], name=\"x-input\")\n",
    "    y_ = tf.placeholder(tf.float32, [None, OUTPUT_NODE], name=\"y-input\")\n",
    "    \n",
    "    # 生成隐藏层参数\n",
    "    weights1 = tf.Variable(tf.truncated_normal([INPUT_NODE, LAYER1_NODE], stddev=0.1))\n",
    "    biases1 = tf.Variable(tf.constant(0.1, shape=[LAYER1_NODE]))\n",
    "    \n",
    "    # 计算当前参数下神经网络的前向传播结果\n",
    "    y = inference(x, None, weights1, biases1)\n",
    "    \n",
    "    # 定义存储训练轮数的变量\n",
    "    # 此变量不需要计算滑动平均值，所以这里指定这个变量为不可训练的变量\n",
    "    # 在tensorflow训练神经网络时，一般会将嗲表训练轮数的变量指定为不可训练的变量\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "    \n",
    "    # --------------------------------------------------------------------------------------------\n",
    "    # 给定滑动平均衰减率和训练轮数的变量，初始化滑动平均类；给定训练轮数的变量可以加快训练早期变量的更新速度\n",
    "    variable_averages = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY, global_step)\n",
    "    # 在所有代表神经网络参数的变量上使用滑动平均（辅助变量不需要）\n",
    "    # tf.trainable_variables返回的就是图上集合GraphKeys.TRAINABLE_AVERAGES中的元素。这个集合的元素就是所有没有指定\n",
    "    # trainable=False的参数\n",
    "    variable_averages_op = variable_averages.apply(tf.trainable_variables())\n",
    "    # 计算使用了滑动平均之后的前向传播结果\n",
    "    average_y = inference(x, variable_averages, weights1, biases1)\n",
    "    \n",
    "    # --------------------------------------------------------------------------------------------\n",
    "    # 计算交叉熵作为刻画预测值和真实值之间差距的损失函数\n",
    "    # 当分类问题只有一个正确答案时，可以使用sparse_softmax_cross_entropy_with_logits函数来加速交叉熵计算\n",
    "    # tf.argmax函数得到正确答案对应的类别编号\n",
    "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(y, tf.argmax(y_, 1))\n",
    "    # 计算在当前batch中所有样例的交叉熵平均值\n",
    "    cross_entropy_mean = tf.reduce_mean(cross_entropy)\n",
    "    \n",
    "    # 计算L2正则化损失函数\n",
    "    regularizer = tf.contrib.layers.l2_regularizer(REGULARIZATION_RATE)\n",
    "    # 计算模型的正则化损失。 一般只计算神经网络边上权重的正则化损失，而不使用偏置项\n",
    "    regularization = regularizer(weights1)\n",
    "    \n",
    "    # 总损失等于交叉熵损失和正则化损失之和\n",
    "    loss = cross_entropy_mean + regularization\n",
    "    \n",
    "    # --------------------------------------------------------------------------------------------\n",
    "    # 设置指数衰减的学习率\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        LEARNING_RATE_BASE,    # 基础学习率，随着迭代的进行，更新变量时使用的学习率在这个基础上递减 \n",
    "        global_step,           # 当前迭代的轮数\n",
    "        mnist.train.num_examples/BATCH_SIZE,    # 总的迭代次数\n",
    "        LEARNING_RATE_DECAY)                    # 学习率递减速度\n",
    "    # 使用tf.train.GradientDescentOptimizer优化算法来优化损失函数\n",
    "    # 这里的损失函数包括了交叉熵损失和L2正则化损失\n",
    "    train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "    \n",
    "    # --------------------------------------------------------------------------------------------\n",
    "    # 在训练神经网络模型时，每过一遍数据既要通过反向传播算法来更新神经网络中的参数，又要更新每一个参数的滑动平均值\n",
    "    # 这一过程，tensorflow提供了tf.control_dependecies和tf.group两种机制\n",
    "    # train_op = tf.group(train_step, variable_averages_op)\n",
    "    with tf.control_dependencies([train_step, variable_averages_op]):\n",
    "        train_op = tf.no_op(name='train')\n",
    "        \n",
    "    # --------------------------------------------------------------------------------------------\n",
    "    # 检验使用了滑动平均模型的神经网络前向传播结果是否正确\n",
    "    # tf.equal 判断两个张量的每一维是否相等，相等返回True，否则返回False\n",
    "    correct_prediction = tf.equal(tf.argmax(average_y, 1), tf.argmax(y_, 1))\n",
    "    # 将bool的数值转为实数型，然后计算平均值。这个平均值就是模型在这一组数据上的正确率\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "    # --------------------------------------------------------------------------------------------\n",
    "    # 初始化会话并开始训练过程\n",
    "    with tf.Session() as sess:\n",
    "        tf.initialize_all_variables().run()\n",
    "        # 准备验证数据。一般在神经网络的训练过程中会通过验证数据来大致判断停止的条件和判断训练的效果\n",
    "        validate_feed = {x: mnist.validation.images, y_: mnist.validation.labels}\n",
    "        # 准备测试数据。在真实的应用中，这部分数据在训练时是不可见的，这个数据只是作为模型优劣的最后评价标准\n",
    "        test_feed = {x: mnist.test.images, y_: mnist.test.labels}\n",
    "        \n",
    "        # 迭代的训练神经网络\n",
    "        for i in range(TRAINING_STEPS):\n",
    "            # 每1000轮输出一次在验证数据集上的测试结果\n",
    "            if i % 1000 == 0:\n",
    "                # 计算滑动平均模型在验证数据上的结果。\n",
    "                # 由于mnist的数据集比较小，所以一次可以处理所有的验证数据。\n",
    "                # 在神经网络比较复杂或者验证数据比较大时，可以将验证数据划分为更新的batch，以免发生内存溢出错误\n",
    "                validate_acc = sess.run(accuracy, feed_dict=validate_feed)\n",
    "                print(\"After %d training step, validation accuracy using average model is: %g\" % (i, validate_acc))\n",
    "            \n",
    "            # 产生这一轮使用的一个batch的训练数据，并运行训练过程\n",
    "            xs, ys = mnist.train.next_batch(BATCH_SIZE)\n",
    "            sess.run(train_op, feed_dict={x: xs, y_: ys})\n",
    "        \n",
    "        # 在训练结束之后，在测试数据上检测神经网络模型的最终正确率\n",
    "        test_acc = sess.run(accuracy, feed_dict=test_feed)\n",
    "        print (\"After %d training step, test accurary using average model is: %g\" % (TRAINING_STEPS, test_acc))\n",
    "        \n",
    "###################################################################################################\n",
    "# 主程序入口\n",
    "def main(argv=None):\n",
    "    mnist = input_data.read_data_sets(\"/Users/xxx/work5/tensorflow/mnist\", one_hot=True)\n",
    "    train(mnist)\n",
    "\n",
    "###################################################################################################\n",
    "# tesorflow提供了一个主程序的入口，tf.app.run会调用上面定义的main函数\n",
    "if __name__ == '__main__':\n",
    "    tf.app.run()\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 神经网络实例---不使用滑动平均"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /Users/xxx/work5/tensorflow/mnist/train-images-idx3-ubyte.gz\n",
      "Extracting /Users/xxx/work5/tensorflow/mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting /Users/xxx/work5/tensorflow/mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting /Users/xxx/work5/tensorflow/mnist/t10k-labels-idx1-ubyte.gz\n",
      "After 0 training step, validation accuracy using average model is: 0.107\n",
      "After 1000 training step, validation accuracy using average model is: 0.9628\n",
      "After 2000 training step, validation accuracy using average model is: 0.9736\n",
      "After 3000 training step, validation accuracy using average model is: 0.979\n",
      "After 4000 training step, validation accuracy using average model is: 0.9836\n",
      "After 5000 training step, test accurary using average model is: 0.9814\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "###################################################################################################\n",
    "# 神经网络实例---不使用滑动平均\n",
    "# tensorflow 使用mnist训练神经网络实例\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "###################################################################################################\n",
    "# mnist 数据集相关常数\n",
    "# 输入层节点数。 对于mnist数据集，这个等于图片的像素\n",
    "INPUT_NODE = 784\n",
    "# 输出层的节点数。这个等于类别的数目\n",
    "OUTPUT_NODE = 10\n",
    "\n",
    "###################################################################################################\n",
    "# 配置神经网络参数\n",
    "# 隐藏层节点数\n",
    "LAYER1_NODE = 500\n",
    "# 一个训练batch中的训练数据个数， batch越大，训练越接近梯度下降；batch越小，训练越接近随机梯度下降\n",
    "BATCH_SIZE = 100\n",
    "# 基础学习率\n",
    "LEARNING_RATE_BASE = 0.8\n",
    "# 学习率衰减率\n",
    "LEARNING_RATE_DECAY = 0.99\n",
    "# 描述模型复杂度的正则化项在损失函数中的系数\n",
    "REGULARIZATION_RATE = 0.0001\n",
    "# 训练论数\n",
    "TRAINING_STEPS = 5000\n",
    "# 滑动平均衰减率\n",
    "MOVING_AVERAGE_DECAY = 0.99\n",
    "\n",
    "###################################################################################################\n",
    "# 辅助函数，给定了神经网络的输入和所有参数，计算神经网络前向传播结果\n",
    "# 使用relu激活函数实现了去线性化， 实现了三层全连接神经网络\n",
    "# 在函数中支持了传入计算参数平均值的类，可在测试时使用滑动平均模型\n",
    "def inference(input_tensor, avg_class, weights1, biases1, weights2, biases2):\n",
    "    # 当没有提供滑动平均类时，直接使用参数当前的取值\n",
    "    if avg_class is None:\n",
    "        # 计算隐藏层的前向传播结果，并且使用relu激活函数\n",
    "        layer1 = tf.nn.relu(tf.matmul(input_tensor, weights1) + biases1)\n",
    "        # 计算输出层的前向传播结果\n",
    "        # 输出层不加入激活函数原因： 在计算损失函数时会一并计算softmax函数\n",
    "        # 不加入softmax不会影响预测结果原因：预测时使用的是不同类别对应节点输出值的相对大小，所以没有softmax层对最后分类结果\n",
    "        # 的计算没有影响。于是在计算整个神经网络的前向传播时可以不加入最后的softmax层\n",
    "        return tf.matmul(layer1, weights2) + biases2\n",
    "    else:\n",
    "        # 首先通过avg_class.average函数计算得出变量的滑动平均值，然后再计算神经网络的前向传播结果\n",
    "        layer1 = tf.nn.relu(tf.matmul(input_tensor, avg_class.average(weights1)) + avg_class.average(biases1))\n",
    "        return tf.matmul(layer1, avg_class.average(weights2)) + avg_class.average(biases2)\n",
    "\n",
    "\n",
    "###################################################################################################\n",
    "# 训练模型\n",
    "def train(mnist):\n",
    "    x = tf.placeholder(tf.float32, [None, INPUT_NODE], name=\"x-input\")\n",
    "    y_ = tf.placeholder(tf.float32, [None, OUTPUT_NODE], name=\"y-input\")\n",
    "    \n",
    "    # 生成隐藏层参数\n",
    "    weights1 = tf.Variable(tf.truncated_normal([INPUT_NODE, LAYER1_NODE], stddev=0.1))\n",
    "    biases1 = tf.Variable(tf.constant(0.1, shape=[LAYER1_NODE]))\n",
    "    # 生成输出层参数\n",
    "    weights2 = tf.Variable(tf.truncated_normal([LAYER1_NODE, OUTPUT_NODE], stddev=0.1))\n",
    "    biases2 = tf.Variable(tf.constant(0.1, shape=[OUTPUT_NODE]))\n",
    "    \n",
    "    # 计算当前参数下神经网络的前向传播结果\n",
    "    y = inference(x, None, weights1, biases1, weights2, biases2)\n",
    "    \n",
    "    # 定义存储训练轮数的变量\n",
    "    # 此变量不需要计算滑动平均值，所以这里指定这个变量为不可训练的变量\n",
    "    # 在tensorflow训练神经网络时，一般会将嗲表训练轮数的变量指定为不可训练的变量\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "    \n",
    "#     # --------------------------------------------------------------------------------------------\n",
    "#     # 给定滑动平均衰减率和训练轮数的变量，初始化滑动平均类；给定训练轮数的变量可以加快训练早期变量的更新速度\n",
    "#     variable_averages = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY, global_step)\n",
    "#     # 在所有代表神经网络参数的变量上使用滑动平均（辅助变量不需要）\n",
    "#     # tf.trainable_variables返回的就是图上集合GraphKeys.TRAINABLE_AVERAGES中的元素。这个集合的元素就是所有没有指定\n",
    "#     # trainable=False的参数\n",
    "#     variable_averages_op = variable_averages.apply(tf.trainable_variables())\n",
    "#     # 计算使用了滑动平均之后的前向传播结果\n",
    "#     average_y = inference(x, variable_averages, weights1, biases1, weights2, biases2)\n",
    "    \n",
    "    # --------------------------------------------------------------------------------------------\n",
    "    # 计算交叉熵作为刻画预测值和真实值之间差距的损失函数\n",
    "    # 当分类问题只有一个正确答案时，可以使用sparse_softmax_cross_entropy_with_logits函数来加速交叉熵计算\n",
    "    # tf.argmax函数得到正确答案对应的类别编号\n",
    "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(y, tf.argmax(y_, 1))\n",
    "    # 计算在当前batch中所有样例的交叉熵平均值\n",
    "    cross_entropy_mean = tf.reduce_mean(cross_entropy)\n",
    "    \n",
    "    # 计算L2正则化损失函数\n",
    "    regularizer = tf.contrib.layers.l2_regularizer(REGULARIZATION_RATE)\n",
    "    # 计算模型的正则化损失。 一般只计算神经网络边上权重的正则化损失，而不使用偏置项\n",
    "    regularization = regularizer(weights1) + regularizer(weights2)\n",
    "    \n",
    "    # 总损失等于交叉熵损失和正则化损失之和\n",
    "    loss = cross_entropy_mean + regularization\n",
    "    \n",
    "    # --------------------------------------------------------------------------------------------\n",
    "    # 设置指数衰减的学习率\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        LEARNING_RATE_BASE,    # 基础学习率，随着迭代的进行，更新变量时使用的学习率在这个基础上递减 \n",
    "        global_step,           # 当前迭代的轮数\n",
    "        mnist.train.num_examples/BATCH_SIZE,    # 总的迭代次数\n",
    "        LEARNING_RATE_DECAY)                    # 学习率递减速度\n",
    "    # 使用tf.train.GradientDescentOptimizer优化算法来优化损失函数\n",
    "    # 这里的损失函数包括了交叉熵损失和L2正则化损失\n",
    "    train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "    \n",
    "    # --------------------------------------------------------------------------------------------\n",
    "    # 在训练神经网络模型时，每过一遍数据既要通过反向传播算法来更新神经网络中的参数，又要更新每一个参数的滑动平均值\n",
    "    # 这一过程，tensorflow提供了tf.control_dependecies和tf.group两种机制\n",
    "    # train_op = tf.group(train_step, variable_averages_op)\n",
    "#     with tf.control_dependencies([train_step, variable_averages_op]):\n",
    "    with tf.control_dependencies([train_step]):\n",
    "        train_op = tf.no_op(name='train')\n",
    "        \n",
    "    # --------------------------------------------------------------------------------------------\n",
    "    # 检验使用了滑动平均模型的神经网络前向传播结果是否正确\n",
    "    # tf.equal 判断两个张量的每一维是否相等，相等返回True，否则返回False\n",
    "#     correct_prediction = tf.equal(tf.argmax(average_y, 1), tf.argmax(y_, 1))\n",
    "    correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "    # 将bool的数值转为实数型，然后计算平均值。这个平均值就是模型在这一组数据上的正确率\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "    # --------------------------------------------------------------------------------------------\n",
    "    # 初始化会话并开始训练过程\n",
    "    with tf.Session() as sess:\n",
    "        tf.initialize_all_variables().run()\n",
    "        # 准备验证数据。一般在神经网络的训练过程中会通过验证数据来大致判断停止的条件和判断训练的效果\n",
    "        validate_feed = {x: mnist.validation.images, y_: mnist.validation.labels}\n",
    "        # 准备测试数据。在真实的应用中，这部分数据在训练时是不可见的，这个数据只是作为模型优劣的最后评价标准\n",
    "        test_feed = {x: mnist.test.images, y_: mnist.test.labels}\n",
    "        \n",
    "        # 迭代的训练神经网络\n",
    "        for i in range(TRAINING_STEPS):\n",
    "            # 每1000轮输出一次在验证数据集上的测试结果\n",
    "            if i % 1000 == 0:\n",
    "                # 计算滑动平均模型在验证数据上的结果。\n",
    "                # 由于mnist的数据集比较小，所以一次可以处理所有的验证数据。\n",
    "                # 在神经网络比较复杂或者验证数据比较大时，可以将验证数据划分为更新的batch，以免发生内存溢出错误\n",
    "                validate_acc = sess.run(accuracy, feed_dict=validate_feed)\n",
    "                print(\"After %d training step, validation accuracy using average model is: %g\" % (i, validate_acc))\n",
    "            \n",
    "            # 产生这一轮使用的一个batch的训练数据，并运行训练过程\n",
    "            xs, ys = mnist.train.next_batch(BATCH_SIZE)\n",
    "            sess.run(train_op, feed_dict={x: xs, y_: ys})\n",
    "        \n",
    "        # 在训练结束之后，在测试数据上检测神经网络模型的最终正确率\n",
    "        test_acc = sess.run(accuracy, feed_dict=test_feed)\n",
    "        print (\"After %d training step, test accurary using average model is: %g\" % (TRAINING_STEPS, test_acc))\n",
    "        \n",
    "###################################################################################################\n",
    "# 主程序入口\n",
    "def main(argv=None):\n",
    "    mnist = input_data.read_data_sets(\"/Users/xxx/work5/tensorflow/mnist\", one_hot=True)\n",
    "    train(mnist)\n",
    "\n",
    "###################################################################################################\n",
    "# tesorflow提供了一个主程序的入口，tf.app.run会调用上面定义的main函数\n",
    "if __name__ == '__main__':\n",
    "    tf.app.run()\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
